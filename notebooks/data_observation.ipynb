{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Comparison of NER methods in the filed of Cyber-Threat-Intelligence\n",
    "\n",
    "## Approach\n",
    "The approach of this report is twofold. In the first instance, the traditional approaches are contrasted. Therefore, the [CoreNLP pipeline](https://stanfordnlp.github.io/CoreNLP/ner.html#additional-tokensregexner-rules), which determines the entities in last instance on the basis [Conditional Random Field (CRF)](https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776), and the [extended spaCy pipeline](https://spacy.io/usage/processing-pipelines) are compared with one another. Extended means that spaCy is the modern one of both libraries and makes it possible for example to replace individual components such as the feature extraction by means of different embedding techniques. Based on this, a [foundation model](https://research.ibm.com/blog/what-are-foundation-models), i.e. a more specialized [transformer pipeline](https://spacy.io/usage/v3#features-transformers-pipelines), is to be integrated into this process and evaluated.\n",
    "\n",
    "## Task definition of NER:\n",
    "NER stands for Named Entity Recognition, which is a subtask of Natural Language Processing (NLP) that involves identifying and categorizing named\n",
    "entities in text into predefined categories such as person names, organization names, locations, and others.\n",
    "\n",
    "## Task definition of NER-CTI\n",
    "NER-CTI stands for Named Entity Recognition for Cyber-Threat-Intelligence, which is a subtask of NER that involves identifying and categorizing named\n",
    "entities related to Cyber-Threats in text into predefined categories such as IPs, URLs, protocols, locations or threat participants.\n",
    "\n",
    "## BIO format\n",
    "The BIO format is a commonly used labeling scheme in NER tasks. In this format, each token in a text is labeled with a prefix indicating whether it belongs to a named entity and, if so, what type of entity it is. The prefix is either \"B\", \"I\", or \"O\", where:\n",
    "\n",
    "B (Beginning) indicates that the token is the beginning of a named entity.\n",
    "I (Inside) indicates that the token is inside a named entity.\n",
    "O (Outside) indicates that the token is not part of a named entity.\n",
    "\n",
    "This is an example of how BIO might look in a sentence:\n",
    "\n",
    "    John   lives in  New   York  City\n",
    "    B-PER  O     O   B-LOC I-LOC I-LOC\n",
    "\n",
    "In this example, \"John\" is the beginning of a person (PER) entity, \"New York\" is the beginning of a location (LOC) entity, and \"City\" is inside the same location entity.\n",
    "\n",
    "## BIOES format\n",
    "The BIOES format is an extension of the BIO format. This format adds more semantic to the respective token-label relation as the simpler BIO format does not consider single words and also tags the last word in an entity with I.\n",
    "With the BIOES format it is possible to denote single entity words like \"John\" (S) and to strictly tell the ending of an entity (E).\n",
    "Hence, the additional prefixes are \"S\" and \"E\", where:\n",
    "\n",
    "S (Start) indicates that the token is the complete named entity.\n",
    "E (Ending) indicates that the token is the ending of a named entity.\n",
    "\n",
    "This is the extended example for BIOES:\n",
    "\n",
    "    John   lives in  New   York  City\n",
    "    S-PER  O     O   B-LOC I-LOC E-LOC\n",
    "\n",
    "However, both formats are interchangeable and the choice how which format to apply depends on how fine-grained the annotation or model should be. But it appears that BIO is preferred for the most applications.\n",
    "\n",
    "## CoNLL format\n",
    "The CoNLL format is a standard format for representing labeled sequences of tokens, often used for tasks like named entity recognition (NER) or part-of-speech (POS) tagging. The format is named after the [Conference on Natural Language Learning (CoNLL)](https://www.conll.org/previous-tasks), which first introduced it in 2000.\n",
    "\n",
    "In the CoNLL format was introduced for the tasks of language-independent named entity recognition in [2002](https://www.clips.uantwerpen.be/conll2002/ner/) and [2003](https://www.clips.uantwerpen.be/conll2003/ner/), each line of a text file represents a single token and its associated labels. The first column contains the token itself, while subsequent columns contain labels for various linguistic features. For example, in a typical NER task, the second column might contain the named entity label for each token, while in a POS tagging task, it might contain the part-of-speech tag.\n",
    "\n",
    "## Data sources:\n",
    "As data is limited for NER-CTI but the question of NER-CTI boils down to the same questions of NER but with special tag-sets, we focus on the following three open-source datasets:\n",
    "\n",
    "| APTNER   | Token    | Unique  | Sentence  | Error |\n",
    "|----------|----------|---------|-----------|-------|\n",
    "| Train    | 154.412  |  11.818 |  6.940    |  518  |\n",
    "| Valid    |  35.990  |   5.501 |  1.664    |   68  |\n",
    "| Test     |  37.359  |   4.793 |  1.529    |   23  |\n",
    "\n",
    "**Entity-Types:** 5  *(B I O E S)*\n",
    "\n",
    "**Entity-Labels:** 22 *('TIME', 'OS', 'ACT', 'LOC', 'TOOL', 'VULNAME', 'DOM', 'APT', 'EMAIL', 'IP', 'SHA1', 'SHA2', 'URL', 'IDTY', 'FILE', 'SECTEAM', 'PROT', 'MAL', 'VULID', 'MD5', 'O', 'ENCR')*\n",
    "\n",
    "**Repository:** https://github.com/wangxuren/APTNER\n",
    "\n",
    "**Example:**\n",
    "\n",
    "    Kaspersky Lab       products detect the malware described\n",
    "    B-SECTEAM E-SECTEAM O        O      O   O       O\n",
    "\n",
    "    in this report as Trojan.Win32.Remexi and Trojan.Win32.Agent .\n",
    "    O  O    O      O  S-FILE              O   S-FILE             0\n",
    "\n",
    "In this example the security team (SECTEAM) \"Kaspersky Lab\" detected the files \"Trojan.Win32.Remexi\" and \"Trojan.Win32.Agent\" (FILE). As it becomes directly apparent by this example, the task is only to identify named entities but no relations between them as \"detect\" is masked as \"O\".\n",
    "\n",
    "-----\n",
    "\n",
    "| CyNER | Token | Unique | Sentence  | Error |\n",
    "|-------|--------|--------|-----------|-------|\n",
    "| Train | 25.769 | 4.567  | 1.097     | 33    |\n",
    "| Valid | 18.742 | 3.363  | 785       | 0     |\n",
    "| Test  | 6.726  | 1.830  | 294       | 12    |\n",
    "\n",
    "**Entity-Types:** 3 *(B I O)*\n",
    "\n",
    "**Entity-Labels:** 6 *('Organization', 'System', 'Malware', 'Indicator', 'O', 'Vulnerability')*\n",
    "\n",
    "**Repository:** https://github.com/aiforsec/CyNER\n",
    "\n",
    "**Example:**\n",
    "\n",
    "    This malicious APK is 334326 bytes file , MD5 :\n",
    "    O    O         O   O  O      O     O    O O   O\n",
    "\n",
    "    0b8806b38b52bebfe39ff585639e2ea2 and is detected\n",
    "    B-Indicator                      O   O  O\n",
    "\n",
    "    by Kaspersky      Lab products   as \" Backdoor.AndroidOS.Chuli.a \" .\n",
    "    O  B-Organization I-Organization O  O B-Indicator                O O\n",
    "\n",
    "This example is similar to the previous one but shows significant differences in expression of a labels meaning. Here, file like \"0b8806b38b52bebfe39ff585639e2ea2\" or Backdoor.AndroidOS.Chuli.a\" (Indicator) are detected by \"Kaspersky Lab\" (Organization). Again, the same relation \"detected by\" is not of further interest.\n",
    "\n",
    "-----\n",
    "\n",
    "| DNRTI    | Token    | Unique | Sentence  | Unique  | Error |\n",
    "|----------|----------|--------|-----------|---------|-------|\n",
    "| Train    | 94.829   | 7.377  | 3.704     | 7.377   |  450  |\n",
    "| Valid    | 16.652   | 3.326  |   662     | 3.326   |   33  |\n",
    "| Test     | 16.706   | 3.239  |   663     | 3.239   |   39  |\n",
    "\n",
    "**Entity-Types:** 3 *(B I O)*\n",
    "\n",
    "**Entity-Labels:** 14 *('Exp', 'OffAct', 'Area', 'SamFile', 'Tool', 'Features', 'Way', 'SecTeam', 'Org', 'Purp', 'Time', 'Idus', 'O', 'HackOrg')*\n",
    "\n",
    "**Repository:** https://github.com/SCreaMxp/DNRTI-A-Large-scale-Dataset-for-Named-Entity-Recognition-in-Threat-Intelligence\n",
    "\n",
    "**Example:**\n",
    "\n",
    "    Kaspersky Lab       's products detect the Microsoft Office exploits\n",
    "    B-SecTeam I-SecTeam  0 0        0      0   B-Exp     I-Exp  I-Exp\n",
    "\n",
    "    used in the spear-phishing attacks  , including Exploit.MSWord.CVE-2010-333\n",
    "    0    0  0   B-OffAct       I-OffAct 0 0         B-SamFile\n",
    "\n",
    "    , Exploit.Win32.CVE-2012-0158 .\n",
    "    0 B-SamFile                   0\n",
    "\n",
    "In this example \"Kaspersky Lab\" (SecTeam) again detect the \"Microsoft Office exploits\" (Exp) that are typical used for offensive acts like \"spear-fishing\" (OffAct) with files such as \"Exploit.MSWord.CVE-2010-333\" and \"Exploit.Win32.CVE-2012-0158\" (SamFile). In this example it is also remarkable that \"'s\" contains more than one token but is only labeled as 'O'.\n",
    "\n",
    "------\n",
    "### About a universal annotation language for CTI data (STIX)\n",
    "\n",
    "As exemplified with the three samples mentioned above, it appears that the might be the need for the yet young community of CTI research to develop a universal annotation language.\n",
    "All the sample mention the same entities, but use different wordings for expressing the same things as this can excellently be traced by files like \"Trojan.Win32.Agent\" tagged as \"FILE\", \"Backdoor.AndroidOS.Chuli.a\" labeled with \"Indicator\", and finally \"Exploit.Win32.CVE-2012-0158\" denoted as \"SamFile\".\n",
    "\n",
    "One aspect might include having a close look at the [STIX](https://oasis-open.github.io/cti-documentation/stix/intro) guidelines mentioned in APTNER (STIX2.1) and DNRTI (STIX).\n",
    "Having a closer look at STIX might also be interesting to find other CTI-datasets also including relationships.\n",
    "\n",
    "Additionally, STIX adds an interesting turn in working with CTI-data by introducing not only \"Entities\" and \"Relations\" but also \"Sightings\" defined as: \"belief that something in CTI (e.g., an indicator, malware, tool, threat actor, etc.) was seen\". This is especially fascinating, because a relation like \"Kaspersky Lab detected Trojan.Win32.Agent\" can be seen as the facts of having a CTI already broke the system. In contrast, a \"sighting\" is information streamed in real-time data not proven to be true or false, thus making the task of detecting cyberattacks especially difficult.\n",
    "\n",
    "### Other (not) usable datasets:\n",
    "* [1TCFII](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1TCFII) contains 1000 binary annotated tweets. This is maybe good for a final model.\n",
    "\n",
    "* [twitter-cyberthreat-detection](https://paperswithcode.com/dataset/twitter-cyberthreat-detection-dataset) contains annotated tweets by their id. Hence, the data is not directly accessible.\n",
    "\n",
    "* [BERT-for-Cybersecurity-NER](https://github.com/stelemate/BERT-for-Cybersecurity-NER) only contains data written in chinese.\n",
    "\n",
    "* [CTIMiner](https://github.com/dgkim0803/CTIMiner) maybe interesting but behind paywall and uses XML structure.\n",
    "\n",
    "* [CrossNER](https://github.com/zliucr/CrossNER) is interesting because it combines entity label from different sources (science, politics, music, ...), good for future work.\n",
    "\n",
    "\n",
    "The following link might be of special interest, as it contains a curated list about resources for CTI in general:\n",
    "* [awesome-threat-intelligence](https://github.com/hslatman/awesome-threat-intelligence)\n",
    "\n",
    "* [Awesome-Cybersecurity-Datasets](https://github.com/shramos/Awesome-Cybersecurity-Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file_from(path):\n",
    "    \"\"\"\n",
    "    This method reads a file for NER-CTI in the format (token, tag) where token and tag are separated by whitespace.\n",
    "    Further, this method counts the cases of data being assigned with more than one label.\n",
    "\n",
    "    :param path: The path to the file to read.\n",
    "    :return: Tuple having all tokens and tags as dataframe and the amount of mislabeled data.\n",
    "    \"\"\"\n",
    "    column_names = ['Token', 'Tag']\n",
    "\n",
    "    with open(path, newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        malicious = 0\n",
    "        token_tag_list = list()\n",
    "        for row in reader:\n",
    "            if len(row) == 1:\n",
    "                row_split = row[0].split()\n",
    "                if len(row_split) == 2:\n",
    "                    token, tag = row_split[0], row_split[1]\n",
    "                    if len(tag.split('-')) == 2 or tag == 'O':\n",
    "                        token_tag_list += [(token, tag)]\n",
    "                    else:\n",
    "                        malicious += 1\n",
    "                else:\n",
    "                    malicious += 1\n",
    "        df = pd.DataFrame.from_records(token_tag_list, columns=column_names)\n",
    "    return df, malicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_readability(dataset, train_mal, valid_mal, test_mal):\n",
    "    \"\"\"\n",
    "    This method shows some information about the readability and error rate regarding a specified dataset.\n",
    "\n",
    "    :param dataset: The dataset to be used.\n",
    "    :param train_mal: Amount of malicious training data.\n",
    "    :param valid_mal: Amount of malicious validation data.\n",
    "    :param test_mal: Amount of malicious test data.\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    train = dataset[dataset.Set == 'train']\n",
    "    valid = dataset[dataset.Set == 'valid']\n",
    "    test = dataset[dataset.Set == 'test']\n",
    "\n",
    "\n",
    "    print(\"Length Train:\", train.shape[0])\n",
    "    print(\"Length Valid:\", valid.shape[0])\n",
    "    print(\"Length Test:\", test.shape[0])\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Sentences Train:\", train[train.Token == '.'].shape[0])\n",
    "    print(\"Sentences Valid:\", valid[valid.Token == '.'].shape[0])\n",
    "    print(\"Sentences Test:\", test[test.Token == '.'].shape[0])\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Unique Tokens Train:\", len(train.Token.unique()))\n",
    "    print(\"Unique Tokens Valid:\", len(valid.Token.unique()))\n",
    "    print(\"Unique Tokens Test:\", len(test.Token.unique()))\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Error Rate Train:\", train_mal)\n",
    "    print(\"Error Rate Dev:\", valid_mal)\n",
    "    print(\"Error Rate Test:\", test_mal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_labels(dataset):\n",
    "    \"\"\"\n",
    "    This method creates an overview about the different kind of labels and types.\n",
    "\n",
    "    :param dataset: The dataset to work with.\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    unique_tags = dataset.Tag.unique()\n",
    "\n",
    "    tag_types = set({'O'})\n",
    "    tag_words = set()\n",
    "\n",
    "    for tag in unique_tags:\n",
    "        tag_type = None\n",
    "        tag_word = None\n",
    "        if '-' in tag:\n",
    "            tag_type = tag.split('-')[0]\n",
    "            tag_word = tag.split('-')[1]\n",
    "            tag_types.update({tag_type})\n",
    "        else:\n",
    "            tag_word = tag\n",
    "\n",
    "        tag_words.update({tag_word})\n",
    "\n",
    "    print('Different Entity Types:', len(tag_types))\n",
    "    print('Different Entity Labels:', len(tag_words))\n",
    "    print('Entity Types:', tag_types)\n",
    "    print('Entity Labels:', tag_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### APTNER ####\n",
      "About the data\n",
      "Length Train: 154412\n",
      "Length Valid: 35990\n",
      "Length Test: 37359\n",
      "\n",
      "Sentences Train: 6940\n",
      "Sentences Valid: 1664\n",
      "Sentences Test: 1529\n",
      "\n",
      "Unique Tokens Train: 11818\n",
      "Unique Tokens Valid: 5501\n",
      "Unique Tokens Test: 4793\n",
      "\n",
      "Error Rate Train: 518\n",
      "Error Rate Dev: 68\n",
      "Error Rate Test: 23\n",
      "\n",
      "About the labels\n",
      "Different Entity Types: 5\n",
      "Different Entity Labels: 22\n",
      "Entity Types: {'B', 'O', 'I', 'E', 'S'}\n",
      "Entity Labels: {'TOOL', 'SECTEAM', 'SHA2', 'ENCR', 'URL', 'MD5', 'IDTY', 'VULID', 'PROT', 'OS', 'LOC', 'IP', 'DOM', 'O', 'EMAIL', 'FILE', 'SHA1', 'ACT', 'TIME', 'APT', 'MAL', 'VULNAME'}\n",
      "\n",
      "#### CyNER ####\n",
      "About the data\n",
      "Length Train: 25769\n",
      "Length Valid: 18742\n",
      "Length Test: 6726\n",
      "\n",
      "Sentences Train: 1097\n",
      "Sentences Valid: 785\n",
      "Sentences Test: 294\n",
      "\n",
      "Unique Tokens Train: 4567\n",
      "Unique Tokens Valid: 3363\n",
      "Unique Tokens Test: 1830\n",
      "\n",
      "Error Rate Train: 33\n",
      "Error Rate Dev: 0\n",
      "Error Rate Test: 12\n",
      "\n",
      "About the labels\n",
      "Different Entity Types: 3\n",
      "Different Entity Labels: 6\n",
      "Entity Types: {'B', 'O', 'I'}\n",
      "Entity Labels: {'System', 'Malware', 'O', 'Indicator', 'Vulnerability', 'Organization'}\n",
      "\n",
      "#### DNRTI ####\n",
      "About the data\n",
      "Length Train: 94829\n",
      "Length Valid: 16652\n",
      "Length Test: 16706\n",
      "\n",
      "Sentences Train: 3704\n",
      "Sentences Valid: 662\n",
      "Sentences Test: 663\n",
      "\n",
      "Unique Tokens Train: 7377\n",
      "Unique Tokens Valid: 3326\n",
      "Unique Tokens Test: 3239\n",
      "\n",
      "Error Rate Train: 450\n",
      "Error Rate Dev: 33\n",
      "Error Rate Test: 39\n",
      "\n",
      "About the labels\n",
      "Different Entity Types: 3\n",
      "Different Entity Labels: 14\n",
      "Entity Types: {'B', 'O', 'I'}\n",
      "Entity Labels: {'SamFile', 'HackOrg', 'Time', 'Exp', 'Org', 'OffAct', 'SecTeam', 'Way', 'Area', 'Purp', 'Idus', 'Tool', 'Features', 'O'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['APTNER', 'CyNER', 'DNRTI']:\n",
    "    train, train_malicious = read_file_from(f'../data/{dataset}/train.txt')\n",
    "    train['Set'] = 'train'\n",
    "    valid, valid_malicious = read_file_from(f'../data/{dataset}/valid.txt')\n",
    "    valid['Set'] = 'valid'\n",
    "    test, test_malicious = read_file_from(f'../data/{dataset}/test.txt')\n",
    "    test['Set'] = 'test'\n",
    "\n",
    "    data = pd.concat([train, valid, test])\n",
    "\n",
    "    print(f'#### {dataset} ####')\n",
    "    print('About the data')\n",
    "    show_readability(data, train_malicious, valid_malicious, test_malicious)\n",
    "    print()\n",
    "    print('About the labels')\n",
    "    show_labels(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation of different NER-techniques:\n",
    "**Idea:** Compare the pipelines of CoreNLP and spaCy by focusing on their primary components. Hence, it might be interesting to see how they both work compared to each other. This means, both have several components leading to the final detection on named entities in texts. Another fascinating factor might be their implementation, usability in terms of programming effort and scalability.\n",
    "\n",
    "**Possible Criteria:**\n",
    "\n",
    "    1) General structure of pipelines\n",
    "    2) Ease of use (Functionality)\n",
    "    3) Changeability of components\n",
    "    4) Domain adaptation\n",
    "    5) Performance (Runtime, Scalability)\n",
    "\n",
    "| Tool                                                                                                                        | Basic Entities | BIO Format | Domain-Adaptation | Methods for Entity Recognition                        | Adding Pre-trained Models | End-to-End Readiness | Programming Language | Popularity on GitHub |\n",
    "|-----------------------------------------------------------------------------------------------------------------------------|----------------|------------|-------------------|-------------------------------------------------------|----------------------------|----------------------|----------------------|----------------------|\n",
    "| [spaCy](https://spacy.io/usage/linguistic-features#named-entities)                                                          | 18             | Yes        | Yes               | Ensemble, CNN, BILSTM, rule-based                     | Yes                        | Yes                  | Python               | 25,000+              |\n",
    "| [flairNLP](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md#named-entity-recognition-ner) | 13             | Yes        | Yes               | Ensemble, CRF, BILSTM, rule-based                     | Yes                        | Yes                  | Python               | 12,000+              |\n",
    "| [NLTK](https://www.nltk.org/book/ch07.html#named-entity-recognition)                                                        | 5              | Yes        | Yes               | MaxEntropy, rule-based, regexp                        | No                         | No                   | Python               | 11,000+              |\n",
    "| [CoreNLP](https://stanfordnlp.github.io/CoreNLP/ner.html)                                                                   | 4              | Yes        | Yes               | Ensemble, CRF, rule-based, perceptron, neural network | No                         | No                   | Java                 | 8,000+               |\n",
    "\n",
    "**spaCy:** spaCy has excellent documentation that is well-organized, comprehensive, and easy to follow. The documentation includes detailed guides for installation, usage, and customization, as well as a complete API reference. Additionally, spaCy has a vibrant community of developers who contribute to the documentation and provide support through forums and chat channels.\n",
    "\n",
    "**flairNLP:** flairNLP also has good documentation, although it is not as extensive as spaCy's. The documentation includes guides for installation, usage, and customization, as well as examples and API reference.\n",
    "\n",
    "**NLTK:** NLTK has been around for a long time and has a very extensive documentation, with comprehensive guides and tutorials for various natural language processing tasks. However, the documentation can be overwhelming for new users, as it covers a lot of ground and may require some programming experience to fully understand.\n",
    "\n",
    "**CoreNLP:** CoreNLP has documentation that is adequate for basic usage, but it can be difficult to navigate and lacks examples and detailed explanations for more advanced features like adding new entities. Additionally, the documentation is less actively maintained than some other libraries, which may make it harder to get support when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# spaCy:\n",
    "\n",
    "The spaCy library provides a powerful and flexible pipeline for state-of-the-art natural language processing. At its core is the nlp object, which represents the pipeline itself. The pipeline is a sequence of tracable components that are applied to each input text in turn, with each component performing a specific task such as tokenization, part-of-speech tagging, or named entity recognition.\n",
    "\n",
    "The nlp object is created by loading a pre-trained model, such as en_core_web_sm, which contains a set of pre-defined pipeline components for standard cases of NER. These components can be modified or extended as needed using the nlp.add_pipe() method. Each pipeline component takes a Doc object as input and returns a modified Doc object with additional annotations.\n",
    "\n",
    "The Doc object represents a processed document of text, and contains a sequence of Token objects that represent individual words or other elements of the text, such as punctuation or whitespace. Each Token object has a variety of properties and annotations, such as its lemma, part-of-speech tag, and named entity label.\n",
    "\n",
    "The nlp object also provides a range of convenient methods and attributes for working with processed documents, such as accessing specific tokens or entities, visualizing the document structure, or performing similarity calculations between documents.\n",
    "\n",
    "![SpaCy pipeline](https://spacy.io/images/pipeline.svg)\n",
    "\n",
    "## Basic Entities Labels\n",
    "\n",
    "All spaCy pipelines provide a basic set of 18 entities to beinterpreted as:\n",
    "\n",
    "    CARDINAL : Numerals that do not fall under another type\n",
    "    DATE : Absolute or relative dates or periods\n",
    "    EVENT : Named hurricanes, battles, wars, sports events, etc.\n",
    "    FAC : Buildings, airports, highways, bridges, etc.\n",
    "    GPE : Countries, cities, states\n",
    "    LANGUAGE : Any named language\n",
    "    LAW : Named documents made into laws.\n",
    "    LOC : Non-GPE locations, mountain ranges, bodies of water\n",
    "    MONEY : Monetary values, including unit\n",
    "    NORP : Nationalities or religious or political groups\n",
    "    ORDINAL : \"first\", \"second\", etc.\n",
    "    ORG : Companies, agencies, institutions, etc.\n",
    "    PERCENT : Percentage, including \"%\"\n",
    "    PERSON : People, including fictional\n",
    "    PRODUCT : Objects, vehicles, foods, etc. (not services)\n",
    "    QUANTITY : Measurements, as of weight or distance\n",
    "    TIME : Times smaller than a day\n",
    "    WORK_OF_ART : Titles of books, songs, etc.\n",
    "\n",
    "These basic entity labels provide a solid ground for the most and common nlp setups.\n",
    "However, certain usecases like NER-CTI other downstream-tasks require specific entity-labels as you can see for APTNER, CyNER and DNRTI. This will be shown in the section \"Domain Adaption\".\n",
    "\n",
    "## Model NER Performance In Comparison\n",
    "\n",
    "| Name                  | F1   | Prec. | Rec. |    \n",
    "|-----------------------|------|-------|------|\n",
    "| en_web_core_sm        | 84.6 | 84.5  | 84.6 |\n",
    "| en_web_core_md        | 85.2 | 84.9  | 85.5 |\n",
    "| en_web_core_lg        | -    | -     | -    |\n",
    "| en_web_core_trf       | -    | -     | -    |\n",
    "\n",
    "## Domain Adaptation\n",
    "TBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Please note: To install nlp-models like en_core_web_md use !python -m spacy download <model>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 11:18:03.532690: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll_tokenizer <function conll_tokenizer at 0x127d2b490>\n",
      "tok2vec <spacy.pipeline.tok2vec.Tok2Vec object at 0x13e6b2c20>\n",
      "tagger <spacy.pipeline.tagger.Tagger object at 0x13e6b1cc0>\n",
      "parser <spacy.pipeline.dep_parser.DependencyParser object at 0x13e5b0c80>\n",
      "senter <spacy.pipeline.senter.SentenceRecognizer object at 0x13e6b2e00>\n",
      "attribute_ruler <spacy.pipeline.attributeruler.AttributeRuler object at 0x13e8a5d80>\n",
      "lemmatizer <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x13e8e0c40>\n",
      "ner <spacy.pipeline.ner.EntityRecognizer object at 0x13e5b0c10>\n",
      "\tLast layer in ner pipeline: linear\n",
      "\n",
      "Named entities in example sentence:\n",
      "Kaspersky Lab PERSON\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define an example sentence from APTNER containing a special component.\n",
    "sentence = \"Kaspersky Lab products detect the malware described in this report as Trojan.Win32.Remexi and Trojan.Win32.Agent .\"\n",
    "\n",
    "# Define the custom tokenizer as a pipeline component\n",
    "@Language.component('conll_tokenizer')\n",
    "def conll_tokenizer(doc):\n",
    "    # Define a regular expression pattern to split tokens on whitespace (CoNLL format) only.\n",
    "    pattern = r'\\s+'\n",
    "    processed_tokens = [token for token in re.split(pattern, doc.text)]\n",
    "    return Doc(doc.vocab, words=processed_tokens)\n",
    "\n",
    "# Loading the ner pipeline.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# The tokenizer is a custom component and runs as a preprocessing pipeline.\n",
    "nlp.add_pipe(\"conll_tokenizer\", first=True)\n",
    "\n",
    "# Have a look at all components present.\n",
    "for name, component in nlp.components:\n",
    "    # Have a look at https://spacy.io/usage/spacy-101#pipelines\n",
    "    # The models components are separate pipelines and can be accessed via nlp.get_pipe(<name>).\n",
    "    print(name, component)\n",
    "\n",
    "# Print the last layer of the ner pipeline used for classification.\n",
    "ner = nlp.get_pipe('ner')\n",
    "ner_last_layer = ner.model.layers[-1]\n",
    "print(\"\\tLast layer in ner pipeline:\", ner_last_layer.name)\n",
    "\n",
    "# Test the custom tokenizer on a sample text\n",
    "print(\"\\nNamed entities in example sentence:\")\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extracting the named entities\n",
    "for ent in nlp(sentence).ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:en_core_web_sm:\n",
      "Source: OntoNotes 5\n",
      "Source: ClearNLP Constituent-to-Dependency Conversion\n",
      "Source: WordNet 3.0\n",
      "NER Performance: \n",
      "\tF1 = 0.846, Prec. = 0.845, Rec. 0.846:\n",
      "\n",
      "Name:en_core_web_md:\n",
      "Source: OntoNotes 5\n",
      "Source: ClearNLP Constituent-to-Dependency Conversion\n",
      "Source: WordNet 3.0\n",
      "Source: Explosion Vectors (OSCAR 2109 + Wikipedia + OpenSubtitles + WMT News Crawl)\n",
      "NER Performance: \n",
      "\tF1 = 0.852, Prec. = 0.849, Rec. 0.855:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model performance on their standard data.\n",
    "for model_name in ['en_core_web_sm', 'en_core_web_md']:\n",
    "    pre_trained_model = spacy.load(model_name)\n",
    "    # get the baseline values for all standard models.\n",
    "    performance = pre_trained_model.meta.get('performance')\n",
    "    pre = performance.get('ents_p')\n",
    "    rec = performance.get('ents_r')\n",
    "    f1 = performance.get('ents_f')\n",
    "    \n",
    "    print(f\"Name:{model_name}:\")\n",
    "    for source in pre_trained_model.meta.get('sources'):\n",
    "        print(f\"Source: {source.get('name')}\")\n",
    "    print(\"NER Performance: \")\n",
    "    print(f\"\\tF1 = {f1:.3f}, Prec. = {pre:.3f}, Rec. {rec:.3f}:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaptation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
