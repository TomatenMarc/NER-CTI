{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Observation of Data related to Cyber-Threat-Intelligence\n",
    "\n",
    "## Task definition of NER:\n",
    "\n",
    "NER stands for Named Entity Recognition, which is a subtask of Natural Language Processing (NLP) that involves identifying and categorizing named\n",
    "entities in text into predefined categories such as person names, organization names, locations, and others.\n",
    "\n",
    "## Task definition of NER-CTI\n",
    "\n",
    "NER-CTI stands for Named Entity Recognition for Cyber-Threat-Intelligence, which is a subtask of NER that involves identifying and categorizing named\n",
    "entities related to Cyber-Threats in text into predefined categories such as IPs, URLs, protocols, locations or threat participants.\n",
    "\n",
    "## Data sources:\n",
    "\n",
    "As data is limited for NER-CTI but the question of NER-CTI boils down to the same questions of NER but with special tag-sets, we focus on the\n",
    "following three open-source datasets:\n",
    "\n",
    "    1. APTNER: https://github.com/wangxuren/APTNER\n",
    "    2. CyNER: https://github.com/aiforsec/CyNER\n",
    "    3. DNRTI: https://github.com/SCreaMxp/DNRTI-A-Large-scale-Dataset-for-Named-Entity-Recognition-in-Threat-Intelligence\n",
    "\n",
    "The following link might be of special interest, as it contains a curated list about resources for CTI in general:\n",
    "\n",
    "    1. https://github.com/hslatman/awesome-threat-intelligence\n",
    "\n",
    "## Comparability (Pre-Limitation):\n",
    "\n",
    "This is a basic comparison of the three datasets APTNER, CyNER and DNRTI according to:\n",
    "\n",
    "    1) Format and readability\n",
    "    2) Data distribution (train, valid, test) at token- and tag-level\n",
    "    3) Interpretation of label semantic\n",
    "\n",
    "### 1) Format and readability\n",
    "\n",
    "Since we work with three datasets, it is important to compare them in terms of some very basic characteristics: format and readability.\n",
    "Format refers to the way the data is structured, meaning if the data formated similar or universal.\n",
    "\n",
    "It turned out that the format is indeed the same. All three files show to have the format (token, whitespace, tag). With respect to APTNER, all\n",
    "datasets use train, valid and test as identifiers. For APTNER the files need to be renamed accordingly. In addition, APTNER uses a dev set which\n",
    "is equivalent to a valid set.\n",
    "\n",
    "In terms of readability, it has to be compared if the data contains any label errors, like tokens only having more than two or no tag.\n",
    "I turned out that the error rate for all three datasets is quite low.\n",
    "\n",
    "## Evaluation of different NER-techniques:\n",
    "\n",
    "    TBC."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def read_file_from(path):\n",
    "    \"\"\"\n",
    "    This method reads a file for NER-CTI in the format (token, tag) where token and tag are separated by whitespace.\n",
    "    Further, this method counts the cases of data being assigned with more than one label.\n",
    "\n",
    "    :param path: The path to the file to read.\n",
    "    :return: Tuple having all tokens and tags as dataframe and the amount of mislabeled data.\n",
    "    \"\"\"\n",
    "    column_names = ['Token', 'Tag']\n",
    "\n",
    "    with open(path, newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        malicious = 0\n",
    "        token_tag_list = list()\n",
    "        for row in reader:\n",
    "            if len(row) == 1:\n",
    "                row_split = row[0].split()\n",
    "                if len(row_split) == 2:\n",
    "                    token, tag = row_split[0], row_split[1]\n",
    "                    token_tag_list += [(token, tag)]\n",
    "                else:\n",
    "                    malicious += 1\n",
    "        df = pd.DataFrame.from_records(token_tag_list, columns=column_names)\n",
    "    return df, malicious"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def show_readability(dataset, train_mal, valid_mal, test_mal):\n",
    "    \"\"\"\n",
    "    This method shows some information about the readability and error rate regarding a specified dataset.\n",
    "\n",
    "    :param dataset: The dataset to be used.\n",
    "    :param train_mal: Amount of malicious training data.\n",
    "    :param valid_mal: Amount of malicious validation data.\n",
    "    :param test_mal: Amount of malicious test data.\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    train = dataset[dataset.Set == 'train']\n",
    "    valid = dataset[dataset.Set == 'valid']\n",
    "    test = dataset[dataset.Set == 'test']\n",
    "\n",
    "\n",
    "    print(\"Length Train:\", train.shape[0])\n",
    "    print(\"Length Valid:\", valid.shape[0])\n",
    "    print(\"Length Test:\", test.shape[0])\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Unique tokens:\", len(dataset.Token.unique()))\n",
    "    print(\"Unique tokens:\", len(train.Token.unique()))\n",
    "    print(\"Unique tokens:\", len(valid.Token.unique()))\n",
    "    print(\"Unique tokens:\", len(test.Token.unique()))\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Error Rate Train:\", round(100 * (train_mal / aptner_train.shape[0]), 2), \"%\")\n",
    "    print(\"Error Rate Dev:\", round(100 * (valid_mal / aptner_valid.shape[0]), 2), \"%\")\n",
    "    print(\"Error Rate Test:\", round(100 * (test_mal / aptner_test.shape[0]), 2), \"%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## APTNER\n",
    "\n",
    "Notice: For this dataset, the files had to be renamed. The original pattern was like \"APTNERtrain.txt\". Now the file are simply train.txt, valid.txt and test.txt. Furthermore, the file \"APTNERdev.txt\" is now valid.txt."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Train: 154426\n",
      "Length Valid: 35990\n",
      "Length Test: 37359\n",
      "\n",
      "Unique tokens: 14991\n",
      "Unique tokens: 11818\n",
      "Unique tokens: 5501\n",
      "Unique tokens: 4793\n",
      "\n",
      "Error Rate Train: 0.33 %\n",
      "Error Rate Dev: 0.19 %\n",
      "Error Rate Test: 0.06 %\n"
     ]
    }
   ],
   "source": [
    "aptner_train, aptner_train_malicious = read_file_from('../data/APTNER/train.txt')\n",
    "aptner_train['Set'] = 'train'\n",
    "aptner_valid, aptner_valid_malicious = read_file_from('../data/APTNER/valid.txt')\n",
    "aptner_valid['Set'] = 'valid'\n",
    "aptner_test, aptner_test_malicious = read_file_from('../data/APTNER/test.txt')\n",
    "aptner_test['Set'] = 'test'\n",
    "\n",
    "aptner = pd.concat([aptner_train, aptner_valid, aptner_test])\n",
    "\n",
    "show_readability(aptner, aptner_train_malicious, aptner_valid_malicious, aptner_test_malicious)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Train: 25769\n",
      "Length Valid: 18742\n",
      "Length Test: 6726\n",
      "\n",
      "Unique tokens: 6733\n",
      "Unique tokens: 4567\n",
      "Unique tokens: 3363\n",
      "Unique tokens: 1830\n",
      "\n",
      "Error Rate Train: 0.02 %\n",
      "Error Rate Dev: 0.0 %\n",
      "Error Rate Test: 0.03 %\n"
     ]
    }
   ],
   "source": [
    "cyner_train, cyner_train_malicious = read_file_from('../data/CyNER/train.txt')\n",
    "cyner_train['Set'] = 'train'\n",
    "cyner_valid, cyner_valid_malicious = read_file_from('../data/CyNER/valid.txt')\n",
    "cyner_valid['Set'] = 'valid'\n",
    "cyner_test, cyner_test_malicious = read_file_from('../data/CyNER/test.txt')\n",
    "cyner_test['Set'] = 'test'\n",
    "\n",
    "cyner = pd.concat([cyner_train, cyner_valid, cyner_test])\n",
    "\n",
    "show_readability(cyner, cyner_train_malicious, cyner_valid_malicious, cyner_test_malicious)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DNRTI"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Train: 94829\n",
      "Length Valid: 16652\n",
      "Length Test: 16706\n",
      "\n",
      "Unique tokens: 8325\n",
      "Unique tokens: 7377\n",
      "Unique tokens: 3326\n",
      "Unique tokens: 3239\n",
      "\n",
      "Error Rate Train: 0.29 %\n",
      "Error Rate Dev: 0.09 %\n",
      "Error Rate Test: 0.1 %\n"
     ]
    }
   ],
   "source": [
    "dnrti_train, dnrti_train_malicious = read_file_from('../data/DNRTI/train.txt')\n",
    "dnrti_train['Set'] = 'train'\n",
    "dnrti_valid, dnrti_valid_malicious = read_file_from('../data/DNRTI/valid.txt')\n",
    "dnrti_valid['Set'] = 'valid'\n",
    "dnrti_test, dnrti_test_malicious = read_file_from('../data/DNRTI/test.txt')\n",
    "dnrti_test['Set'] = 'test'\n",
    "\n",
    "dnrti = pd.concat([dnrti_train, dnrti_valid, dnrti_test])\n",
    "\n",
    "show_readability(dnrti, dnrti_train_malicious, dnrti_valid_malicious, dnrti_test_malicious)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}