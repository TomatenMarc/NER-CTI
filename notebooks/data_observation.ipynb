{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Comparison of NER methods in the filed of Cyber-Threat-Intelligence\n",
    "\n",
    "## Approach\n",
    "The approach of this report is twofold. In the first instance, the traditional approaches are contrasted. Therefore, the [CoreNLP pipeline](https://stanfordnlp.github.io/CoreNLP/ner.html#additional-tokensregexner-rules), which determines the entities in last instance on the basis [Conditional Random Field (CRF)](https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776), and the [extended spaCy pipeline](https://spacy.io/usage/processing-pipelines) are compared with one another. Extended means that spaCy is the modern one of both libraries and makes it possible for example to replace individual components such as the feature extraction by means of different embedding techniques. Based on this, a [foundation model](https://research.ibm.com/blog/what-are-foundation-models), i.e. a more specialized [transformer pipeline](https://spacy.io/usage/v3#features-transformers-pipelines), is to be integrated into this process and evaluated.\n",
    "\n",
    "## Task definition of NER:\n",
    "NER stands for Named Entity Recognition, which is a subtask of Natural Language Processing (NLP) that involves identifying and categorizing named\n",
    "entities in text into predefined categories such as person names, organization names, locations, and others.\n",
    "\n",
    "## Task definition of NER-CTI\n",
    "NER-CTI stands for Named Entity Recognition for Cyber-Threat-Intelligence, which is a subtask of NER that involves identifying and categorizing named\n",
    "entities related to Cyber-Threats in text into predefined categories such as IPs, URLs, protocols, locations or threat participants.\n",
    "\n",
    "## BIO format\n",
    "The BIO format is a commonly used labeling scheme in NER tasks. In this format, each token in a text is labeled with a prefix indicating whether it belongs to a named entity and, if so, what type of entity it is. The prefix is either \"B\", \"I\", or \"O\", where:\n",
    "\n",
    "B (Beginning) indicates that the token is the beginning of a named entity.\n",
    "I (Inside) indicates that the token is inside a named entity.\n",
    "O (Outside) indicates that the token is not part of a named entity.\n",
    "\n",
    "This is an example of how BIO might look in a sentence:\n",
    "\n",
    "    John   lives in  New   York  City\n",
    "    B-PER  O     O   B-LOC I-LOC I-LOC\n",
    "\n",
    "In this example, \"John\" is the beginning of a person (PER) entity, \"New York\" is the beginning of a location (LOC) entity, and \"City\" is inside the same location entity.\n",
    "\n",
    "## BIOES format\n",
    "The BIOES format is an extension of the BIO format. This format adds more semantic to the respective token-label relation as the simpler BIO format does not consider single words and also tags the last word in an entity with I.\n",
    "With the BIOES format it is possible to denote single entity words like \"John\" (S) and to strictly tell the ending of an entity (E).\n",
    "Hence, the additional prefixes are \"S\" and \"E\", where:\n",
    "\n",
    "S (Start) indicates that the token is the complete named entity.\n",
    "E (Ending) indicates that the token is the ending of a named entity.\n",
    "\n",
    "This is the extended example for BIOES:\n",
    "\n",
    "    John   lives in  New   York  City\n",
    "    S-PER  O     O   B-LOC I-LOC E-LOC\n",
    "\n",
    "However, both formats are interchangeable and the choice how which format to apply depends on how fine-grained the annotation or model should be. But it appears that BIO is preferred for the most applications.\n",
    "\n",
    "## CoNLL format\n",
    "The CoNLL format is a standard format for representing labeled sequences of tokens, often used for tasks like named entity recognition (NER) or part-of-speech (POS) tagging. The format is named after the Conference on Natural Language Learning (CoNLL), which first introduced it in 2000.\n",
    "\n",
    "In the CoNLL format, each line of a text file represents a single token and its associated labels. The first column contains the token itself, while subsequent columns contain labels for various linguistic features. For example, in a typical NER task, the second column might contain the named entity label for each token, while in a POS tagging task, it might contain the part-of-speech tag.\n",
    "\n",
    "## Data sources:\n",
    "As data is limited for NER-CTI but the question of NER-CTI boils down to the same questions of NER but with special tag-sets, we focus on the following three open-source datasets:\n",
    "\n",
    "| APTNER   | Token    | Unique  | Sentence  | Error |\n",
    "|----------|----------|---------|-----------|-------|\n",
    "| Train    | 154.412  |  11.818 |  6.940    |  518  |\n",
    "| Valid    |  35.990  |   5.501 |  1.664    |   68  |\n",
    "| Test     |  37.359  |   4.793 |  1.529    |   23  |\n",
    "\n",
    "**Entity-Types:** 5  *(B I O E S)*\n",
    "\n",
    "**Entity-Labels:** 22 *('TIME', 'OS', 'ACT', 'LOC', 'TOOL', 'VULNAME', 'DOM', 'APT', 'EMAIL', 'IP', 'SHA1', 'SHA2', 'URL', 'IDTY', 'FILE', 'SECTEAM', 'PROT', 'MAL', 'VULID', 'MD5', 'O', 'ENCR')*\n",
    "\n",
    "**Repository:** https://github.com/wangxuren/APTNER\n",
    "\n",
    "**Example:**\n",
    "\n",
    "    Kaspersky Lab       products detect the malware described in this report as Trojan.Win32.Remexi and Trojan.Win32.Agent .\n",
    "    B-SECTEAM E-SECTEAM O        O      O   O       O         O  O    O      O  S-FILE              O   S-FILE             0\n",
    "\n",
    "In this example the security team (SECTEAM) \"Kaspersky Lab\" detected the files \"Trojan.Win32.Remexi\" and \"Trojan.Win32.Agent\" (FILE). As it becomes directly apparent by this example, the task is only to identify named entities but no relations between them as \"detect\" is masked as \"O\".\n",
    "\n",
    "-----\n",
    "\n",
    "| CyNER | Token | Unique | Sentence  | Error |\n",
    "|-------|--------|--------|-----------|-------|\n",
    "| Train | 25.769 | 4.567  | 1.097     | 33    |\n",
    "| Valid | 18.742 | 3.363  | 785       | 0     |\n",
    "| Test  | 6.726  | 1.830  | 294       | 12    |\n",
    "\n",
    "**Entity-Types:** 3 *(B I O)*\n",
    "\n",
    "**Entity-Labels:** 6 *('Organization', 'System', 'Malware', 'Indicator', 'O', 'Vulnerability')*\n",
    "\n",
    "**Repository:** https://github.com/aiforsec/CyNER\n",
    "\n",
    "**Example:**\n",
    "\n",
    "    This malicious APK is 334326 bytes file , MD5 : 0b8806b38b52bebfe39ff585639e2ea2 and is detected by Kaspersky      Lab products   as \" Backdoor.AndroidOS.Chuli.a \" .\n",
    "    O    O         O   O  O      O     O    O O   O B-Indicator                      O   O  O        O  B-Organization I-Organization O  O B-Indicator                O O\n",
    "\n",
    "This example is similar to the previous one but shows significant differences in expression of a labels meaning. Here, file like \"0b8806b38b52bebfe39ff585639e2ea2\" or Backdoor.AndroidOS.Chuli.a\" (Indicator) are detected by \"Kaspersky Lab\" (Organization). Again, the same relation \"detected by\" is not of further interest.\n",
    "\n",
    "-----\n",
    "\n",
    "| DNRTI    | Token    | Unique | Sentence  | Unique  | Error |\n",
    "|----------|----------|--------|-----------|---------|-------|\n",
    "| Train    | 94.829   | 7.377  | 3.704     | 7.377   |  450  |\n",
    "| Valid    | 16.652   | 3.326  |   662     | 3.326   |   33  |\n",
    "| Test     | 16.706   | 3.239  |   663     | 3.239   |   39  |\n",
    "\n",
    "**Entity-Types:** 3 *(B I O)*\n",
    "\n",
    "**Entity-Labels:** 14 *('Exp', 'OffAct', 'Area', 'SamFile', 'Tool', 'Features', 'Way', 'SecTeam', 'Org', 'Purp', 'Time', 'Idus', 'O', 'HackOrg')*\n",
    "\n",
    "**Repository:** https://github.com/SCreaMxp/DNRTI-A-Large-scale-Dataset-for-Named-Entity-Recognition-in-Threat-Intelligence\n",
    "\n",
    "**Example:**\n",
    "\n",
    "    Kaspersky Lab       's products detect the Microsoft Office exploits used in the spear-phishing attacks  , including Exploit.MSWord.CVE-2010-333 , Exploit.Win32.CVE-2012-0158 .\n",
    "    B-SecTeam I-SecTeam  0 0        0      0   B-Exp     I-Exp  I-Exp    0    0  0   B-OffAct       I-OffAct 0 0         B-SamFile                   0 B-SamFile                   0\n",
    "\n",
    "In this example \"Kaspersky Lab\" (SecTeam) again detect the \"Microsoft Office exploits\" (Exp) that are typical used for offensive acts like \"spear-fishing\" (OffAct) with files such as \"Exploit.MSWord.CVE-2010-333\" and \"Exploit.Win32.CVE-2012-0158\" (SamFile). In this example it is also remarkable that \"'s\" contains more than one token but is only labeled as 'O'.\n",
    "\n",
    "------\n",
    "### About a universal annotation language for CTI data (STIX)\n",
    "\n",
    "As exemplified with the three samples mentioned above, it appears that the might be the need for the yet young community of CTI research to develop a universal annotation language.\n",
    "All the sample mention the same entities, but use different wordings for expressing the same things as this can excellently be traced by files like \"Trojan.Win32.Agent\" tagged as \"FILE\", \"Backdoor.AndroidOS.Chuli.a\" labeled with \"Indicator\", and finally \"Exploit.Win32.CVE-2012-0158\" denoted as \"SamFile\".\n",
    "\n",
    "One aspect might include having a close look at the [STIX](https://oasis-open.github.io/cti-documentation/stix/intro) guidelines mentioned in APTNER (STIX2.1) and DNRTI (STIX).\n",
    "Having a closer look at STIX might also be interesting to find other CTI-datasets also including relationships.\n",
    "\n",
    "Additionally, STIX adds an interesting turn in working with CTI-data by introducing not only \"Entities\" and \"Relations\" but also \"Sightings\" defined as: \"belief that something in CTI (e.g., an indicator, malware, tool, threat actor, etc.) was seen\". This is especially fascinating, because a relation like \"Kaspersky Lab detected Trojan.Win32.Agent\" can be seen as the facts of having a CTI already broke the system. In contrast, a \"sighting\" is information streamed in real-time data not proven to be true or false, thus making the task of detecting cyberattacks especially difficult.\n",
    "\n",
    "### Other (not) usable datasets:\n",
    "* [1TCFII](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1TCFII) contains 1000 binary annotated tweets. This is maybe good for a final model.\n",
    "\n",
    "* [twitter-cyberthreat-detection](https://paperswithcode.com/dataset/twitter-cyberthreat-detection-dataset) contains annotated tweets by their id. Hence, the data is not directly accessible.\n",
    "\n",
    "* [BERT-for-Cybersecurity-NER](https://github.com/stelemate/BERT-for-Cybersecurity-NER) only contains data written in chinese.\n",
    "\n",
    "* [CTIMiner](https://github.com/dgkim0803/CTIMiner) maybe interesting but behind paywall and uses XML structure.\n",
    "\n",
    "* [CrossNER](https://github.com/zliucr/CrossNER) is interesting because it combines entity label from different sources (science, politics, music, ...), good for future work.\n",
    "\n",
    "\n",
    "The following link might be of special interest, as it contains a curated list about resources for CTI in general:\n",
    "* [awesome-threat-intelligence](https://github.com/hslatman/awesome-threat-intelligence)\n",
    "\n",
    "* [Awesome-Cybersecurity-Datasets](https://github.com/shramos/Awesome-Cybersecurity-Datasets)\n",
    "\n",
    "\n",
    "## Evaluation of different NER-techniques:\n",
    "**Idea:** Compare the pipelines of CoreNLP and spaCy by focusing on their primary components. Hence, it might be interesting to see how they both work compared to each other. This means, both have several components leading to the final detection on named entities in texts. Another fascinating factor might be their implementation, usability in terms of programming effort and scalability.\n",
    "\n",
    "**Possible Criteria:**\n",
    "\n",
    "    1) General structure of pipelines\n",
    "    2) Ease of use (Functionality)\n",
    "    3) Changeability of components\n",
    "    4) Domain adaptation\n",
    "    5) Performance (Runtime, Scalability)\n",
    "\n",
    "| Tool                                                                                                                        | Basic Entities | BIO Format | Domain-Adaptation | Methods for Entity Recognition                        | Adding Pre-trained Models | End-to-End Readiness | Programming Language | Popularity on GitHub |\n",
    "|-----------------------------------------------------------------------------------------------------------------------------|----------------|------------|-------------------|-------------------------------------------------------|----------------------------|----------------------|----------------------|----------------------|\n",
    "| [spaCy](https://spacy.io/usage/linguistic-features#named-entities)                                                          | 18             | Yes        | Yes               | Ensemble, CNN, BILSTM, rule-based                     | Yes                        | Yes                  | Python               | 25,000+              |\n",
    "| [flairNLP](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md#named-entity-recognition-ner) | 13             | Yes        | Yes               | Ensemble, CRF, BILSTM, rule-based                     | Yes                        | Yes                  | Python               | 12,000+              |\n",
    "| [NLTK](https://www.nltk.org/book/ch07.html#named-entity-recognition)                                                        | 5              | Yes        | Yes               | MaxEntropy, rule-based, regexp                        | No                         | No                   | Python               | 11,000+              |\n",
    "| [CoreNLP](https://stanfordnlp.github.io/CoreNLP/ner.html)                                                                   | 4              | Yes        | Yes               | Ensemble, CRF, rule-based, perceptron, neural network | No                         | No                   | Java                 | 8,000+               |\n",
    "\n",
    "**spaCy:** spaCy has excellent documentation that is well-organized, comprehensive, and easy to follow. The documentation includes detailed guides for installation, usage, and customization, as well as a complete API reference. Additionally, spaCy has a vibrant community of developers who contribute to the documentation and provide support through forums and chat channels.\n",
    "\n",
    "**flairNLP:** flairNLP also has good documentation, although it is not as extensive as spaCy's. The documentation includes guides for installation, usage, and customization, as well as examples and API reference.\n",
    "\n",
    "**NLTK:** NLTK has been around for a long time and has a very extensive documentation, with comprehensive guides and tutorials for various natural language processing tasks. However, the documentation can be overwhelming for new users, as it covers a lot of ground and may require some programming experience to fully understand.\n",
    "\n",
    "**CoreNLP:** CoreNLP has documentation that is adequate for basic usage, but it can be difficult to navigate and lacks examples and detailed explanations for more advanced features like adding new entities. Additionally, the documentation is less actively maintained than some other libraries, which may make it harder to get support when needed.\n",
    "\n",
    "https://www.oreilly.com/library/view/natural-language-processing/9781789130386/a5542c53-6df4-4945-a67e-eb6556d108cf.xhtml\n",
    "\n",
    "https://medium.com/activewizards-machine-learning-company/comparison-of-top-6-python-nlp-libraries-c4ce160237eb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file_from(path):\n",
    "    \"\"\"\n",
    "    This method reads a file for NER-CTI in the format (token, tag) where token and tag are separated by whitespace.\n",
    "    Further, this method counts the cases of data being assigned with more than one label.\n",
    "\n",
    "    :param path: The path to the file to read.\n",
    "    :return: Tuple having all tokens and tags as dataframe and the amount of mislabeled data.\n",
    "    \"\"\"\n",
    "    column_names = ['Token', 'Tag']\n",
    "\n",
    "    with open(path, newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        malicious = 0\n",
    "        token_tag_list = list()\n",
    "        for row in reader:\n",
    "            if len(row) == 1:\n",
    "                row_split = row[0].split()\n",
    "                if len(row_split) == 2:\n",
    "                    token, tag = row_split[0], row_split[1]\n",
    "                    if len(tag.split('-')) == 2 or tag == 'O':\n",
    "                        token_tag_list += [(token, tag)]\n",
    "                    else:\n",
    "                        malicious += 1\n",
    "                else:\n",
    "                    malicious += 1\n",
    "        df = pd.DataFrame.from_records(token_tag_list, columns=column_names)\n",
    "    return df, malicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_readability(dataset, train_mal, valid_mal, test_mal):\n",
    "    \"\"\"\n",
    "    This method shows some information about the readability and error rate regarding a specified dataset.\n",
    "\n",
    "    :param dataset: The dataset to be used.\n",
    "    :param train_mal: Amount of malicious training data.\n",
    "    :param valid_mal: Amount of malicious validation data.\n",
    "    :param test_mal: Amount of malicious test data.\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    train = dataset[dataset.Set == 'train']\n",
    "    valid = dataset[dataset.Set == 'valid']\n",
    "    test = dataset[dataset.Set == 'test']\n",
    "\n",
    "\n",
    "    print(\"Length Train:\", train.shape[0])\n",
    "    print(\"Length Valid:\", valid.shape[0])\n",
    "    print(\"Length Test:\", test.shape[0])\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Sentences Train:\", train[train.Token == '.'].shape[0])\n",
    "    print(\"Sentences Valid:\", valid[valid.Token == '.'].shape[0])\n",
    "    print(\"Sentences Test:\", test[test.Token == '.'].shape[0])\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Unique Tokens Train:\", len(train.Token.unique()))\n",
    "    print(\"Unique Tokens Valid:\", len(valid.Token.unique()))\n",
    "    print(\"Unique Tokens Test:\", len(test.Token.unique()))\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Error Rate Train:\", train_mal)\n",
    "    print(\"Error Rate Dev:\", valid_mal)\n",
    "    print(\"Error Rate Test:\", test_mal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_labels(dataset):\n",
    "    \"\"\"\n",
    "    This method creates an overview about the different kind of labels and types.\n",
    "\n",
    "    :param dataset: The dataset to work with.\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    unique_tags = dataset.Tag.unique()\n",
    "\n",
    "    tag_types = set({'O'})\n",
    "    tag_words = set()\n",
    "\n",
    "    for tag in unique_tags:\n",
    "        tag_type = None\n",
    "        tag_word = None\n",
    "        if '-' in tag:\n",
    "            tag_type = tag.split('-')[0]\n",
    "            tag_word = tag.split('-')[1]\n",
    "            tag_types.update({tag_type})\n",
    "        else:\n",
    "            tag_word = tag\n",
    "\n",
    "        tag_words.update({tag_word})\n",
    "\n",
    "    print('Different Entity Types:', len(tag_types))\n",
    "    print('Different Entity Labels:', len(tag_words))\n",
    "    print('Entity Types:', tag_types)\n",
    "    print('Entity Labels:', tag_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### APTNER ####\n",
      "About the data\n",
      "Length Train: 154412\n",
      "Length Valid: 35990\n",
      "Length Test: 37359\n",
      "\n",
      "Sentences Train: 6940\n",
      "Sentences Valid: 1664\n",
      "Sentences Test: 1529\n",
      "\n",
      "Unique Tokens Train: 11818\n",
      "Unique Tokens Valid: 5501\n",
      "Unique Tokens Test: 4793\n",
      "\n",
      "Error Rate Train: 518\n",
      "Error Rate Dev: 68\n",
      "Error Rate Test: 23\n",
      "\n",
      "About the labels\n",
      "Different Entity Types: 5\n",
      "Different Entity Labels: 22\n",
      "Entity Types: {'B', 'E', 'S', 'O', 'I'}\n",
      "Entity Labels: {'O', 'MAL', 'FILE', 'TOOL', 'ENCR', 'SECTEAM', 'SHA2', 'SHA1', 'VULNAME', 'IDTY', 'DOM', 'TIME', 'VULID', 'URL', 'LOC', 'MD5', 'ACT', 'OS', 'APT', 'PROT', 'IP', 'EMAIL'}\n",
      "\n",
      "#### CyNER ####\n",
      "About the data\n",
      "Length Train: 25769\n",
      "Length Valid: 18742\n",
      "Length Test: 6726\n",
      "\n",
      "Sentences Train: 1097\n",
      "Sentences Valid: 785\n",
      "Sentences Test: 294\n",
      "\n",
      "Unique Tokens Train: 4567\n",
      "Unique Tokens Valid: 3363\n",
      "Unique Tokens Test: 1830\n",
      "\n",
      "Error Rate Train: 33\n",
      "Error Rate Dev: 0\n",
      "Error Rate Test: 12\n",
      "\n",
      "About the labels\n",
      "Different Entity Types: 3\n",
      "Different Entity Labels: 6\n",
      "Entity Types: {'B', 'I', 'O'}\n",
      "Entity Labels: {'Indicator', 'Organization', 'Vulnerability', 'O', 'System', 'Malware'}\n",
      "\n",
      "#### DNRTI ####\n",
      "About the data\n",
      "Length Train: 94829\n",
      "Length Valid: 16652\n",
      "Length Test: 16706\n",
      "\n",
      "Sentences Train: 3704\n",
      "Sentences Valid: 662\n",
      "Sentences Test: 663\n",
      "\n",
      "Unique Tokens Train: 7377\n",
      "Unique Tokens Valid: 3326\n",
      "Unique Tokens Test: 3239\n",
      "\n",
      "Error Rate Train: 450\n",
      "Error Rate Dev: 33\n",
      "Error Rate Test: 39\n",
      "\n",
      "About the labels\n",
      "Different Entity Types: 3\n",
      "Different Entity Labels: 14\n",
      "Entity Types: {'B', 'I', 'O'}\n",
      "Entity Labels: {'Area', 'SamFile', 'Tool', 'O', 'Way', 'Org', 'OffAct', 'Exp', 'Idus', 'Purp', 'Time', 'HackOrg', 'SecTeam', 'Features'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['APTNER', 'CyNER', 'DNRTI']:\n",
    "    train, train_malicious = read_file_from(f'../data/{dataset}/train.txt')\n",
    "    train['Set'] = 'train'\n",
    "    valid, valid_malicious = read_file_from(f'../data/{dataset}/valid.txt')\n",
    "    valid['Set'] = 'valid'\n",
    "    test, test_malicious = read_file_from(f'../data/{dataset}/test.txt')\n",
    "    test['Set'] = 'test'\n",
    "\n",
    "    data = pd.concat([train, valid, test])\n",
    "\n",
    "    print(f'#### {dataset} ####')\n",
    "    print('About the data')\n",
    "    show_readability(data, train_malicious, valid_malicious, test_malicious)\n",
    "    print()\n",
    "    print('About the labels')\n",
    "    show_labels(data)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}