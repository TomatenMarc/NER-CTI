{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Observation of Data related to Cyber-Threat-Intelligence\n",
    "\n",
    "## Task definition of NER:\n",
    "\n",
    "NER stands for Named Entity Recognition, which is a subtask of Natural Language Processing (NLP) that involves identifying and categorizing named\n",
    "entities in text into predefined categories such as person names, organization names, locations, and others.\n",
    "\n",
    "## Task definition of NER-CTI\n",
    "\n",
    "NER-CTI stands for Named Entity Recognition for Cyber-Threat-Intelligence, which is a subtask of NER that involves identifying and categorizing named\n",
    "entities related to Cyber-Threats in text into predefined categories such as IPs, URLs, protocols, locations or threat participants.\n",
    "\n",
    "## BIO format\n",
    "The BIO format is a commonly used labeling scheme in NER tasks. In this format, each token in a text is labeled with a prefix indicating whether it belongs to a named entity and, if so, what type of entity it is. The prefix is either \"B\", \"I\", or \"O\", where:\n",
    "\n",
    "B (Beginning) indicates that the token is the beginning of a named entity.\n",
    "I (Inside) indicates that the token is inside a named entity.\n",
    "O (Outside) indicates that the token is not part of a named entity.\n",
    "\n",
    "This is an example of how BIO might look in a sentence:\n",
    "\n",
    "    John   lives in  New   York  City\n",
    "    B-PER  O     O   B-LOC I-LOC I-LOC\n",
    "\n",
    "In this example, \"John\" is the beginning of a person (PER) entity, \"New York\" is the beginning of a location (LOC) entity, and \"City\" is inside the same location entity.\n",
    "\n",
    "## CoNLL format\n",
    "\n",
    "The CoNLL format is a standard format for representing labeled sequences of tokens, often used for tasks like named entity recognition (NER) or part-of-speech (POS) tagging. The format is named after the Conference on Natural Language Learning (CoNLL), which first introduced it in 2000.\n",
    "\n",
    "In the CoNLL format, each line of a text file represents a single token and its associated labels. The first column contains the token itself, while subsequent columns contain labels for various linguistic features. For example, in a typical NER task, the second column might contain the named entity label for each token, while in a POS tagging task, it might contain the part-of-speech tag.\n",
    "\n",
    "## Data sources:\n",
    "\n",
    "As data is limited for NER-CTI but the question of NER-CTI boils down to the same questions of NER but with special tag-sets, we focus on the\n",
    "following three open-source datasets:\n",
    "\n",
    "| APTNER   | Token    | Unique  | Sentence  | Error |\n",
    "|----------|----------|---------|-----------|-------|\n",
    "| Train    | 154.412  |  11.818 |  6.940    |  518  |\n",
    "| Valid    |  35.990  |   5.501 |  1.664    |   68  |\n",
    "| Test     |  37.359  |   4.793 |  1.529    |   23  |\n",
    "\n",
    "**Entity-Types:** 5  *(B I O E S)*\n",
    "\n",
    "**Entity-Labels:** 22 *('TIME', 'OS', 'ACT', 'LOC', 'TOOL', 'VULNAME', 'DOM', 'APT', 'EMAIL', 'IP', 'SHA1', 'SHA2', 'URL', 'IDTY', 'FILE', 'SECTEAM', 'PROT', 'MAL', 'VULID', 'MD5', 'O', 'ENCR')*\n",
    "\n",
    "**Repository:** https://github.com/wangxuren/APTNER\n",
    "\n",
    "| CyNER | Token | Unique | Sentence  | Error |\n",
    "|-------|--------|--------|-----------|-------|\n",
    "| Train | 25.769 | 4.567  | 1.097     | 33    |\n",
    "| Valid | 18.742 | 3.363  | 785       | 0     |\n",
    "| Test  | 6.726  | 1.830  | 294       | 12    |\n",
    "\n",
    "**Entity-Types:** 3 *(B I O)*\n",
    "\n",
    "**Entity-Labels:** 6 *('Organization', 'System', 'Malware', 'Indicator', 'O', 'Vulnerability')*\n",
    "\n",
    "**Repository:** https://github.com/aiforsec/CyNER\n",
    "\n",
    "| DNRTI    | Token    | Unique | Sentence  | Unique  | Error |\n",
    "|----------|----------|--------|-----------|---------|-------|\n",
    "| Train    | 94.829   | 7.377  | 3.704     | 7.377   |  450  |\n",
    "| Valid    | 16.652   | 3.326  |   662     | 3.326   |   33  |\n",
    "| Test     | 16.706   | 3.239  |   663     | 3.239   |   39  |\n",
    "\n",
    "**Entity-Types:** 3 *(B I O)*\n",
    "\n",
    "**Entity-Labels:** 14 *('Exp', 'OffAct', 'Area', 'SamFile', 'Tool', 'Features', 'Way', 'SecTeam', 'Org', 'Purp', 'Time', 'Idus', 'O', 'HackOrg')*\n",
    "\n",
    "**Repository:** https://github.com/SCreaMxp/DNRTI-A-Large-scale-Dataset-for-Named-Entity-Recognition-in-Threat-Intelligence\n",
    "\n",
    "### Other (not) usable datasets:\n",
    "\n",
    "\n",
    "* [1TCFII](!https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1TCFII) contains 1000 binary annotated tweets. This is maybe good for a final model.\n",
    "\n",
    "* [twitter-cyberthreat-detection](!https://paperswithcode.com/dataset/twitter-cyberthreat-detection-dataset) contains annotated tweets by their id. Hence, the data is not directly accessible.\n",
    "\n",
    "* [BERT-for-Cybersecurity-NER](!https://github.com/stelemate/BERT-for-Cybersecurity-NER) only contains data written in chinese.\n",
    "\n",
    "* [CTIMiner](!https://github.com/dgkim0803/CTIMiner) maybe interesting but behind paywall and uses XML structure.\n",
    "\n",
    "* [CrossNER](!https://github.com/zliucr/CrossNER) is interesting because it combines entity label from different sources (science, politics, music, ...), good for future work.\n",
    "\n",
    "\n",
    "The following link might be of special interest, as it contains a curated list about resources for CTI in general:\n",
    "\n",
    "* [awesome-threat-intelligence](!https://github.com/hslatman/awesome-threat-intelligence)\n",
    "\n",
    "* [Awesome-Cybersecurity-Datasets](!https://github.com/shramos/Awesome-Cybersecurity-Datasets)\n",
    "\n",
    "\n",
    "## Evaluation of different NER-techniques:\n",
    "\n",
    "**Idea:** Compare the pipelines of CoreNLP and spaCy by focusing on their primary components. Hence, it might be interesting to see how they both work compared to each other. This means, both have several components leading to the final detection on named entities in texts. Another interesting factor might be their implementation, usability in terms of programming effort and scalability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file_from(path):\n",
    "    \"\"\"\n",
    "    This method reads a file for NER-CTI in the format (token, tag) where token and tag are separated by whitespace.\n",
    "    Further, this method counts the cases of data being assigned with more than one label.\n",
    "\n",
    "    :param path: The path to the file to read.\n",
    "    :return: Tuple having all tokens and tags as dataframe and the amount of mislabeled data.\n",
    "    \"\"\"\n",
    "    column_names = ['Token', 'Tag']\n",
    "\n",
    "    with open(path, newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        malicious = 0\n",
    "        token_tag_list = list()\n",
    "        for row in reader:\n",
    "            if len(row) == 1:\n",
    "                row_split = row[0].split()\n",
    "                if len(row_split) == 2:\n",
    "                    token, tag = row_split[0], row_split[1]\n",
    "                    if len(tag.split('-')) == 2 or tag == 'O':\n",
    "                        token_tag_list += [(token, tag)]\n",
    "                    else:\n",
    "                        malicious += 1\n",
    "                else:\n",
    "                    malicious += 1\n",
    "        df = pd.DataFrame.from_records(token_tag_list, columns=column_names)\n",
    "    return df, malicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_readability(dataset, train_mal, valid_mal, test_mal):\n",
    "    \"\"\"\n",
    "    This method shows some information about the readability and error rate regarding a specified dataset.\n",
    "\n",
    "    :param dataset: The dataset to be used.\n",
    "    :param train_mal: Amount of malicious training data.\n",
    "    :param valid_mal: Amount of malicious validation data.\n",
    "    :param test_mal: Amount of malicious test data.\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    train = dataset[dataset.Set == 'train']\n",
    "    valid = dataset[dataset.Set == 'valid']\n",
    "    test = dataset[dataset.Set == 'test']\n",
    "\n",
    "\n",
    "    print(\"Length Train:\", train.shape[0])\n",
    "    print(\"Length Valid:\", valid.shape[0])\n",
    "    print(\"Length Test:\", test.shape[0])\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Sentences Train:\", train[train.Token == '.'].shape[0])\n",
    "    print(\"Sentences Valid:\", valid[valid.Token == '.'].shape[0])\n",
    "    print(\"Sentences Test:\", test[test.Token == '.'].shape[0])\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Unique Tokens Train:\", len(train.Token.unique()))\n",
    "    print(\"Unique Tokens Valid:\", len(valid.Token.unique()))\n",
    "    print(\"Unique Tokens Test:\", len(test.Token.unique()))\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Error Rate Train:\", train_mal)\n",
    "    print(\"Error Rate Dev:\", valid_mal)\n",
    "    print(\"Error Rate Test:\", test_mal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_labels(dataset):\n",
    "    \"\"\"\n",
    "    This method creates an overview about the different kind of labels and types.\n",
    "\n",
    "    :param dataset: The dataset to work with.\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    unique_tags = dataset.Tag.unique()\n",
    "\n",
    "    tag_types = set({'O'})\n",
    "    tag_words = set()\n",
    "\n",
    "    for tag in unique_tags:\n",
    "        tag_type = None\n",
    "        tag_word = None\n",
    "        if '-' in tag:\n",
    "            tag_type = tag.split('-')[0]\n",
    "            tag_word = tag.split('-')[1]\n",
    "            tag_types.update({tag_type})\n",
    "        else:\n",
    "            tag_word = tag\n",
    "\n",
    "        tag_words.update({tag_word})\n",
    "\n",
    "    print('Different Entity Types:', len(tag_types))\n",
    "    print('Different Entity Labels:', len(tag_words))\n",
    "    print('Entity Types:', tag_types)\n",
    "    print('Entity Labels:', tag_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### APTNER ####\n",
      "About the data\n",
      "Length Train: 154412\n",
      "Length Valid: 35990\n",
      "Length Test: 37359\n",
      "\n",
      "Sentences Train: 6940\n",
      "Sentences Valid: 1664\n",
      "Sentences Test: 1529\n",
      "\n",
      "Unique Tokens Train: 11818\n",
      "Unique Tokens Valid: 5501\n",
      "Unique Tokens Test: 4793\n",
      "\n",
      "Error Rate Train: 518\n",
      "Error Rate Dev: 68\n",
      "Error Rate Test: 23\n",
      "\n",
      "About the labels\n",
      "Different Entity Types: 5\n",
      "Different Entity Labels: 22\n",
      "Entity Types: {'B', 'E', 'S', 'O', 'I'}\n",
      "Entity Labels: {'O', 'MAL', 'FILE', 'TOOL', 'ENCR', 'SECTEAM', 'SHA2', 'SHA1', 'VULNAME', 'IDTY', 'DOM', 'TIME', 'VULID', 'URL', 'LOC', 'MD5', 'ACT', 'OS', 'APT', 'PROT', 'IP', 'EMAIL'}\n",
      "\n",
      "#### CyNER ####\n",
      "About the data\n",
      "Length Train: 25769\n",
      "Length Valid: 18742\n",
      "Length Test: 6726\n",
      "\n",
      "Sentences Train: 1097\n",
      "Sentences Valid: 785\n",
      "Sentences Test: 294\n",
      "\n",
      "Unique Tokens Train: 4567\n",
      "Unique Tokens Valid: 3363\n",
      "Unique Tokens Test: 1830\n",
      "\n",
      "Error Rate Train: 33\n",
      "Error Rate Dev: 0\n",
      "Error Rate Test: 12\n",
      "\n",
      "About the labels\n",
      "Different Entity Types: 3\n",
      "Different Entity Labels: 6\n",
      "Entity Types: {'B', 'I', 'O'}\n",
      "Entity Labels: {'Indicator', 'Organization', 'Vulnerability', 'O', 'System', 'Malware'}\n",
      "\n",
      "#### DNRTI ####\n",
      "About the data\n",
      "Length Train: 94829\n",
      "Length Valid: 16652\n",
      "Length Test: 16706\n",
      "\n",
      "Sentences Train: 3704\n",
      "Sentences Valid: 662\n",
      "Sentences Test: 663\n",
      "\n",
      "Unique Tokens Train: 7377\n",
      "Unique Tokens Valid: 3326\n",
      "Unique Tokens Test: 3239\n",
      "\n",
      "Error Rate Train: 450\n",
      "Error Rate Dev: 33\n",
      "Error Rate Test: 39\n",
      "\n",
      "About the labels\n",
      "Different Entity Types: 3\n",
      "Different Entity Labels: 14\n",
      "Entity Types: {'B', 'I', 'O'}\n",
      "Entity Labels: {'Area', 'SamFile', 'Tool', 'O', 'Way', 'Org', 'OffAct', 'Exp', 'Idus', 'Purp', 'Time', 'HackOrg', 'SecTeam', 'Features'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['APTNER', 'CyNER', 'DNRTI']:\n",
    "    train, train_malicious = read_file_from(f'../data/{dataset}/train.txt')\n",
    "    train['Set'] = 'train'\n",
    "    valid, valid_malicious = read_file_from(f'../data/{dataset}/valid.txt')\n",
    "    valid['Set'] = 'valid'\n",
    "    test, test_malicious = read_file_from(f'../data/{dataset}/test.txt')\n",
    "    test['Set'] = 'test'\n",
    "\n",
    "    data = pd.concat([train, valid, test])\n",
    "\n",
    "    print(f'#### {dataset} ####')\n",
    "    print('About the data')\n",
    "    show_readability(data, train_malicious, valid_malicious, test_malicious)\n",
    "    print()\n",
    "    print('About the labels')\n",
    "    show_labels(data)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}