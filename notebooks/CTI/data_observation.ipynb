{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Comparison of NER methods in the filed of Cyber-Threat-Intelligence\n",
    "\n",
    "## Approach\n",
    "The approach of this report is twofold. In the first instance, the traditional approaches are contrasted. Therefore, the [CoreNLP pipeline](https://stanfordnlp.github.io/CoreNLP/ner.html#additional-tokensregexner-rules), which determines the entities in last instance on the basis [Conditional Random Field (CRF)](https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776), and the [extended spaCy pipeline](https://spacy.io/usage/processing-pipelines) are compared with one another. Extended means that spaCy is the modern one of both libraries and makes it possible for example to replace individual components such as the feature extraction by means of different embedding techniques. Based on this, a [foundation model](https://research.ibm.com/blog/what-are-foundation-models), i.e. a more specialized [transformer pipeline](https://spacy.io/usage/v3#features-transformers-pipelines), is to be integrated into this process and evaluated.\n",
    "\n",
    "## Task definition of NER:\n",
    "NER stands for Named Entity Recognition, which is a subtask of Natural Language Processing (NLP) that involves identifying and categorizing named\n",
    "entities in text into predefined categories such as person names, organization names, locations, and others.\n",
    "\n",
    "## Task definition of NER-CTI\n",
    "NER-CTI stands for Named Entity Recognition for Cyber-Threat-Intelligence, which is a subtask of NER that involves identifying and categorizing named\n",
    "entities related to Cyber-Threats in text into predefined categories such as IPs, URLs, protocols, locations or threat participants.\n",
    "\n",
    "## BIO format\n",
    "The BIO format is a commonly used labeling scheme in NER tasks. In this format, each token in a text is labeled with a prefix indicating whether it belongs to a named entity and, if so, what type of entity it is. The prefix is either \"B\", \"I\", or \"O\", where:\n",
    "\n",
    "B (Beginning) indicates that the token is the beginning of a named entity.\n",
    "I (Inside) indicates that the token is inside a named entity.\n",
    "O (Outside) indicates that the token is not part of a named entity.\n",
    "\n",
    "This is an example of how BIO might look in a sentence:\n",
    "\n",
    "    John   lives in  New   York  City\n",
    "    B-PER  O     O   B-LOC I-LOC I-LOC\n",
    "\n",
    "In this example, \"John\" is the beginning of a person (PER) entity, \"New York\" is the beginning of a location (LOC) entity, and \"City\" is inside the same location entity.\n",
    "\n",
    "## BIOES format\n",
    "The BIOES format is an extension of the BIO format. This format adds more semantic to the respective token-label relation as the simpler BIO format does not consider single words and also tags the last word in an entity with I.\n",
    "With the BIOES format it is possible to denote single entity words like \"John\" (S) and to strictly tell the ending of an entity (E).\n",
    "Hence, the additional prefixes are \"S\" and \"E\", where:\n",
    "\n",
    "S (Start) indicates that the token is the complete named entity.\n",
    "E (Ending) indicates that the token is the ending of a named entity.\n",
    "\n",
    "This is the extended example for BIOES:\n",
    "\n",
    "    John   lives in  New   York  City\n",
    "    S-PER  O     O   B-LOC I-LOC E-LOC\n",
    "\n",
    "However, both formats are interchangeable and the choice how which format to apply depends on how fine-grained the annotation or model should be. But it appears that BIO is preferred for the most applications.\n",
    "\n",
    "## CoNLL format\n",
    "The CoNLL format is a standard format for representing labeled sequences of tokens, often used for tasks like named entity recognition (NER) or part-of-speech (POS) tagging. The format is named after the [Conference on Natural Language Learning (CoNLL)](https://www.conll.org/previous-tasks), which first introduced it in 2000.\n",
    "\n",
    "In the CoNLL format was introduced for the tasks of language-independent named entity recognition in [2002](https://www.clips.uantwerpen.be/conll2002/ner/) and [2003](https://www.clips.uantwerpen.be/conll2003/ner/), each line of a text file represents a single token and its associated labels. The first column contains the token itself, while subsequent columns contain labels for various linguistic features. For example, in a typical NER task, the second column might contain the named entity label for each token, while in a POS tagging task, it might contain the part-of-speech tag.\n",
    "\n",
    "## Data sources:\n",
    "As data is limited for NER-CTI but the question of NER-CTI boils down to the same questions of NER but with special tag-sets, we focus on the following three open-source datasets:\n",
    "\n",
    "| APTNER   | Token    | Unique  | Sentence  | Error |\n",
    "|----------|----------|---------|-----------|-------|\n",
    "| Train    | 154.412  |  11.818 |  6.940    |  518  |\n",
    "| Valid    |  35.990  |   5.501 |  1.664    |   68  |\n",
    "| Test     |  37.359  |   4.793 |  1.529    |   23  |\n",
    "\n",
    "**Entity-Types:** 5  *(B I O E S)*\n",
    "\n",
    "**Entity-Labels:** 22 *('TIME', 'OS', 'ACT', 'LOC', 'TOOL', 'VULNAME', 'DOM', 'APT', 'EMAIL', 'IP', 'SHA1', 'SHA2', 'URL', 'IDTY', 'FILE', 'SECTEAM', 'PROT', 'MAL', 'VULID', 'MD5', 'O', 'ENCR')*\n",
    "\n",
    "**Repository:** https://github.com/wangxuren/APTNER\n",
    "\n",
    "**Example:**\n",
    "\n",
    "    Kaspersky Lab       products detect the malware described\n",
    "    B-SECTEAM E-SECTEAM O        O      O   O       O\n",
    "\n",
    "    in this report as Trojan.Win32.Remexi and Trojan.Win32.Agent .\n",
    "    O  O    O      O  S-FILE              O   S-FILE             0\n",
    "\n",
    "In this example the security team (SECTEAM) \"Kaspersky Lab\" detected the files \"Trojan.Win32.Remexi\" and \"Trojan.Win32.Agent\" (FILE). As it becomes directly apparent by this example, the task is only to identify named entities but no relations between them as \"detect\" is masked as \"O\".\n",
    "\n",
    "-----\n",
    "\n",
    "| CyNER | Token | Unique | Sentence  | Error |\n",
    "|-------|--------|--------|-----------|-------|\n",
    "| Train | 25.769 | 4.567  | 1.097     | 33    |\n",
    "| Valid | 18.742 | 3.363  | 785       | 0     |\n",
    "| Test  | 6.726  | 1.830  | 294       | 12    |\n",
    "\n",
    "**Entity-Types:** 3 *(B I O)*\n",
    "\n",
    "**Entity-Labels:** 6 *('Organization', 'System', 'Malware', 'Indicator', 'O', 'Vulnerability')*\n",
    "\n",
    "**Repository:** https://github.com/aiforsec/CyNER\n",
    "\n",
    "**Example:**\n",
    "\n",
    "    This malicious APK is 334326 bytes file , MD5 :\n",
    "    O    O         O   O  O      O     O    O O   O\n",
    "\n",
    "    0b8806b38b52bebfe39ff585639e2ea2 and is detected\n",
    "    B-Indicator                      O   O  O\n",
    "\n",
    "    by Kaspersky      Lab products   as \" Backdoor.AndroidOS.Chuli.a \" .\n",
    "    O  B-Organization I-Organization O  O B-Indicator                O O\n",
    "\n",
    "This example is similar to the previous one but shows significant differences in expression of a labels meaning. Here, file like \"0b8806b38b52bebfe39ff585639e2ea2\" or Backdoor.AndroidOS.Chuli.a\" (Indicator) are detected by \"Kaspersky Lab\" (Organization). Again, the same relation \"detected by\" is not of further interest.\n",
    "\n",
    "-----\n",
    "\n",
    "| DNRTI    | Token    | Unique | Sentence  | Unique  | Error |\n",
    "|----------|----------|--------|-----------|---------|-------|\n",
    "| Train    | 94.829   | 7.377  | 3.704     | 7.377   |  450  |\n",
    "| Valid    | 16.652   | 3.326  |   662     | 3.326   |   33  |\n",
    "| Test     | 16.706   | 3.239  |   663     | 3.239   |   39  |\n",
    "\n",
    "**Entity-Types:** 3 *(B I O)*\n",
    "\n",
    "**Entity-Labels:** 14 *('Exp', 'OffAct', 'Area', 'SamFile', 'Tool', 'Features', 'Way', 'SecTeam', 'Org', 'Purp', 'Time', 'Idus', 'O', 'HackOrg')*\n",
    "\n",
    "**Repository:** https://github.com/SCreaMxp/DNRTI-A-Large-scale-Dataset-for-Named-Entity-Recognition-in-Threat-Intelligence\n",
    "\n",
    "**Example:**\n",
    "\n",
    "    Kaspersky Lab       's products detect the Microsoft Office exploits\n",
    "    B-SecTeam I-SecTeam  0 0        0      0   B-Exp     I-Exp  I-Exp\n",
    "\n",
    "    used in the spear-phishing attacks  , including Exploit.MSWord.CVE-2010-333\n",
    "    0    0  0   B-OffAct       I-OffAct 0 0         B-SamFile\n",
    "\n",
    "    , Exploit.Win32.CVE-2012-0158 .\n",
    "    0 B-SamFile                   0\n",
    "\n",
    "In this example \"Kaspersky Lab\" (SecTeam) again detect the \"Microsoft Office exploits\" (Exp) that are typical used for offensive acts like \"spear-fishing\" (OffAct) with files such as \"Exploit.MSWord.CVE-2010-333\" and \"Exploit.Win32.CVE-2012-0158\" (SamFile). In this example it is also remarkable that \"'s\" contains more than one token but is only labeled as 'O'.\n",
    "\n",
    "------\n",
    "### About a universal annotation language for CTI data (STIX)\n",
    "\n",
    "As exemplified with the three samples mentioned above, it appears that the might be the need for the yet young community of CTI research to develop a universal annotation language.\n",
    "All the sample mention the same entities, but use different wordings for expressing the same things as this can excellently be traced by files like \"Trojan.Win32.Agent\" tagged as \"FILE\", \"Backdoor.AndroidOS.Chuli.a\" labeled with \"Indicator\", and finally \"Exploit.Win32.CVE-2012-0158\" denoted as \"SamFile\".\n",
    "\n",
    "One aspect might include having a close look at the [STIX](https://oasis-open.github.io/cti-documentation/stix/intro) guidelines mentioned in APTNER (STIX2.1) and DNRTI (STIX).\n",
    "Having a closer look at STIX might also be interesting to find other CTI-datasets also including relationships.\n",
    "\n",
    "Additionally, STIX adds an interesting turn in working with CTI-data by introducing not only \"Entities\" and \"Relations\" but also \"Sightings\" defined as: \"belief that something in CTI (e.g., an indicator, malware, tool, threat actor, etc.) was seen\". This is especially fascinating, because a relation like \"Kaspersky Lab detected Trojan.Win32.Agent\" can be seen as the facts of having a CTI already broke the system. In contrast, a \"sighting\" is information streamed in real-time data not proven to be true or false, thus making the task of detecting cyberattacks especially difficult.\n",
    "\n",
    "### Other (not) usable datasets:\n",
    "* [1TCFII](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1TCFII) contains 1000 binary annotated tweets. This is maybe good for a final model.\n",
    "\n",
    "* [twitter-cyberthreat-detection](https://paperswithcode.com/dataset/twitter-cyberthreat-detection-dataset) contains annotated tweets by their id. Hence, the data is not directly accessible.\n",
    "\n",
    "* [BERT-for-Cybersecurity-NER](https://github.com/stelemate/BERT-for-Cybersecurity-NER) only contains data written in chinese.\n",
    "\n",
    "* [CTIMiner](https://github.com/dgkim0803/CTIMiner) maybe interesting but behind paywall and uses XML structure.\n",
    "\n",
    "* [CrossNER](https://github.com/zliucr/CrossNER) is interesting because it combines entity label from different sources (science, politics, music, ...), good for future work.\n",
    "\n",
    "\n",
    "The following link might be of special interest, as it contains a curated list about resources for CTI in general:\n",
    "* [awesome-threat-intelligence](https://github.com/hslatman/awesome-threat-intelligence)\n",
    "\n",
    "* [Awesome-Cybersecurity-Datasets](https://github.com/shramos/Awesome-Cybersecurity-Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file_from(path):\n",
    "    \"\"\"\n",
    "    This method reads a file for NER-CTI in the format (token, tag) where token and tag are separated by whitespace.\n",
    "    Further, this method counts the cases of data being assigned with more than one label.\n",
    "\n",
    "    :param path: The path to the file to read.\n",
    "    :return: Tuple having all tokens and tags as dataframe and the amount of mislabeled data.\n",
    "    \"\"\"\n",
    "    column_names = ['Token', 'Tag']\n",
    "\n",
    "    with open(path, newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        malicious = 0\n",
    "        token_tag_list = list()\n",
    "        for row in reader:\n",
    "            if len(row) == 1:\n",
    "                row_split = row[0].split()\n",
    "                if len(row_split) == 2:\n",
    "                    token, tag = row_split[0], row_split[1]\n",
    "                    if len(tag.split('-')) == 2 or tag == 'O':\n",
    "                        token_tag_list += [(token, tag)]\n",
    "                    else:\n",
    "                        malicious += 1\n",
    "                else:\n",
    "                    malicious += 1\n",
    "        df = pd.DataFrame.from_records(token_tag_list, columns=column_names)\n",
    "    return df, malicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_readability(dataset, train_mal, valid_mal, test_mal):\n",
    "    \"\"\"\n",
    "    This method shows some information about the readability and error rate regarding a specified dataset.\n",
    "\n",
    "    :param dataset: The dataset to be used.\n",
    "    :param train_mal: Amount of malicious training data.\n",
    "    :param valid_mal: Amount of malicious validation data.\n",
    "    :param test_mal: Amount of malicious test data.\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "\n",
    "    train = dataset[dataset.Set == 'train']\n",
    "    valid = dataset[dataset.Set == 'valid']\n",
    "    test = dataset[dataset.Set == 'test']\n",
    "\n",
    "\n",
    "    print(\"Length Train:\", train.shape[0])\n",
    "    print(\"Length Valid:\", valid.shape[0])\n",
    "    print(\"Length Test:\", test.shape[0])\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Sentences Train:\", train[train.Token == '.'].shape[0])\n",
    "    print(\"Sentences Valid:\", valid[valid.Token == '.'].shape[0])\n",
    "    print(\"Sentences Test:\", test[test.Token == '.'].shape[0])\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Unique Tokens Train:\", len(train.Token.unique()))\n",
    "    print(\"Unique Tokens Valid:\", len(valid.Token.unique()))\n",
    "    print(\"Unique Tokens Test:\", len(test.Token.unique()))\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Error Rate Train:\", train_mal)\n",
    "    print(\"Error Rate Dev:\", valid_mal)\n",
    "    print(\"Error Rate Test:\", test_mal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_labels(dataset):\n",
    "    \"\"\"\n",
    "    This method creates an overview about the different kind of labels and types.\n",
    "\n",
    "    :param dataset: The dataset to work with.\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    unique_tags = dataset.Tag.unique()\n",
    "\n",
    "    tag_types = set({'O'})\n",
    "    tag_words = set()\n",
    "\n",
    "    for tag in unique_tags:\n",
    "        tag_type = None\n",
    "        tag_word = None\n",
    "        if '-' in tag:\n",
    "            tag_type = tag.split('-')[0]\n",
    "            tag_word = tag.split('-')[1]\n",
    "            tag_types.update({tag_type})\n",
    "        else:\n",
    "            tag_word = tag\n",
    "\n",
    "        tag_words.update({tag_word})\n",
    "\n",
    "    print('Different Entity Types:', len(tag_types))\n",
    "    print('Different Entity Labels:', len(tag_words))\n",
    "    print('Entity Types:', tag_types)\n",
    "    print('Entity Labels:', tag_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for dataset in ['APTNER', 'CyNER', 'DNRTI']:\n",
    "    train, train_malicious = read_file_from(f'../data/{dataset}/train.txt')\n",
    "    train['Set'] = 'train'\n",
    "    valid, valid_malicious = read_file_from(f'../data/{dataset}/valid.txt')\n",
    "    valid['Set'] = 'valid'\n",
    "    test, test_malicious = read_file_from(f'../data/{dataset}/test.txt')\n",
    "    test['Set'] = 'test'\n",
    "\n",
    "    data = pd.concat([train, valid, test])\n",
    "\n",
    "    print(f'#### {dataset} ####')\n",
    "    print('About the data')\n",
    "    show_readability(data, train_malicious, valid_malicious, test_malicious)\n",
    "    print()\n",
    "    print('About the labels')\n",
    "    show_labels(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation of different NER-techniques:\n",
    "**Idea:** Compare the pipelines of CoreNLP and spaCy by focusing on their primary components. Hence, it might be interesting to see how they both work compared to each other. This means, both have several components leading to the final detection on named entities in texts. Another fascinating factor might be their implementation, usability in terms of programming effort and scalability.\n",
    "\n",
    "**Possible Criteria:**\n",
    "\n",
    "    1) General structure of pipelines\n",
    "    2) Ease of use (Functionality)\n",
    "    3) Changeability of components\n",
    "    4) Domain adaptation\n",
    "    5) Performance (Runtime, Scalability)\n",
    "\n",
    "| Tool                                                                                                                        | Basic Entities | BIO Format | Domain-Adaptation | Methods for Entity Recognition                        | Adding Pre-trained Models | End-to-End Readiness | Programming Language | Popularity on GitHub |\n",
    "|-----------------------------------------------------------------------------------------------------------------------------|----------------|------------|-------------------|-------------------------------------------------------|----------------------------|----------------------|----------------------|----------------------|\n",
    "| [spaCy](https://spacy.io/usage/linguistic-features#named-entities)                                                          | 18             | Yes        | Yes               | Ensemble, CNN, BILSTM, rule-based                     | Yes                        | Yes                  | Python               | 25,000+              |\n",
    "| [flairNLP](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md#named-entity-recognition-ner) | 13             | Yes        | Yes               | Ensemble, CRF, BILSTM, rule-based                     | Yes                        | Yes                  | Python               | 12,000+              |\n",
    "| [NLTK](https://www.nltk.org/book/ch07.html#named-entity-recognition)                                                        | 5              | Yes        | Yes               | MaxEntropy, rule-based, regexp                        | No                         | No                   | Python               | 11,000+              |\n",
    "| [CoreNLP](https://stanfordnlp.github.io/CoreNLP/ner.html)                                                                   | 4              | Yes        | Yes               | Ensemble, CRF, rule-based, perceptron, neural network | No                         | No                   | Java                 | 8,000+               |\n",
    "\n",
    "**spaCy:** spaCy has excellent documentation that is well-organized, comprehensive, and easy to follow. The documentation includes detailed guides for installation, usage, and customization, as well as a complete API reference. Additionally, spaCy has a vibrant community of developers who contribute to the documentation and provide support through forums and chat channels.\n",
    "\n",
    "**flairNLP:** flairNLP also has good documentation, although it is not as extensive as spaCy's. The documentation includes guides for installation, usage, and customization, as well as examples and API reference.\n",
    "\n",
    "**NLTK:** NLTK has been around for a long time and has a very extensive documentation, with comprehensive guides and tutorials for various natural language processing tasks. However, the documentation can be overwhelming for new users, as it covers a lot of ground and may require some programming experience to fully understand.\n",
    "\n",
    "**CoreNLP:** CoreNLP has documentation that is adequate for basic usage, but it can be difficult to navigate and lacks examples and detailed explanations for more advanced features like adding new entities. Additionally, the documentation is less actively maintained than some other libraries, which may make it harder to get support when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# spaCy:\n",
    "\n",
    "The spaCy library provides a powerful and flexible pipeline for state-of-the-art natural language processing. At its core is the nlp object, which represents the pipeline itself. The pipeline is a sequence of tracable components that are applied to each input text in turn, with each component performing a specific task such as tokenization, part-of-speech tagging, or named entity recognition.\n",
    "\n",
    "The nlp object is created by loading a pre-trained model, such as en_core_web_sm, which contains a set of pre-defined pipeline components for standard cases of NER. These components can be modified or extended as needed using the nlp.add_pipe() method. Each pipeline component takes a Doc object as input and returns a modified Doc object with additional annotations.\n",
    "\n",
    "The Doc object represents a processed document of text, and contains a sequence of Token objects that represent individual words or other elements of the text, such as punctuation or whitespace. Each Token object has a variety of properties and annotations, such as its lemma, part-of-speech tag, and named entity label.\n",
    "\n",
    "The nlp object also provides a range of convenient methods and attributes for working with processed documents, such as accessing specific tokens or entities, visualizing the document structure, or performing similarity calculations between documents.\n",
    "\n",
    "![SpaCy pipeline](https://spacy.io/images/pipeline.svg)\n",
    "\n",
    "## Basic Entities Labels\n",
    "\n",
    "All spaCy pipelines provide a basic set of 18 entities to beinterpreted as:\n",
    "\n",
    "    CARDINAL : Numerals that do not fall under another type\n",
    "    DATE : Absolute or relative dates or periods\n",
    "    EVENT : Named hurricanes, battles, wars, sports events, etc.\n",
    "    FAC : Buildings, airports, highways, bridges, etc.\n",
    "    GPE : Countries, cities, states\n",
    "    LANGUAGE : Any named language\n",
    "    LAW : Named documents made into laws.\n",
    "    LOC : Non-GPE locations, mountain ranges, bodies of water\n",
    "    MONEY : Monetary values, including unit\n",
    "    NORP : Nationalities or religious or political groups\n",
    "    ORDINAL : \"first\", \"second\", etc.\n",
    "    ORG : Companies, agencies, institutions, etc.\n",
    "    PERCENT : Percentage, including \"%\"\n",
    "    PERSON : People, including fictional\n",
    "    PRODUCT : Objects, vehicles, foods, etc. (not services)\n",
    "    QUANTITY : Measurements, as of weight or distance\n",
    "    TIME : Times smaller than a day\n",
    "    WORK_OF_ART : Titles of books, songs, etc.\n",
    "\n",
    "These basic entity labels provide a solid ground for the most and common nlp setups.\n",
    "However, certain usecases like NER-CTI other downstream-tasks require specific entity-labels as you can see for APTNER, CyNER and DNRTI. This will be shown in the section \"Domain Adaption\".\n",
    "\n",
    "## Model NER Performance In Comparison\n",
    "\n",
    "| Name                  | F1   | Prec. | Rec. |    \n",
    "|-----------------------|------|-------|------|\n",
    "| en_web_core_sm        | 84.6 | 84.5  | 84.6 |\n",
    "| en_web_core_md        | 85.2 | 84.9  | 85.5 |\n",
    "| en_web_core_lg        | -    | -     | -    |\n",
    "| en_web_core_trf       | -    | -     | -    |\n",
    "\n",
    "## Domain Adaptation\n",
    "TBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Please note: To install nlp-models like en_core_web_md use !python -m spacy download <model>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define an example sentence from APTNER containing a special component.\n",
    "sentence = \"Kaspersky Lab products detect the malware described in this report as Trojan.Win32.Remexi and Trojan.Win32.Agent .\"\n",
    "\n",
    "# Define the custom tokenizer as a pipeline component\n",
    "@Language.component('conll_tokenizer')\n",
    "def conll_tokenizer(doc):\n",
    "    # Define a regular expression pattern to split tokens on whitespace (CoNLL format) only.\n",
    "    pattern = r'\\s+'\n",
    "    processed_tokens = [token for token in re.split(pattern, doc.text)]\n",
    "    return Doc(doc.vocab, words=processed_tokens)\n",
    "\n",
    "# Loading the ner pipeline.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# The tokenizer is a custom component and runs as a preprocessing pipeline.\n",
    "nlp.add_pipe(\"conll_tokenizer\", first=True)\n",
    "\n",
    "# Have a look at all components present.\n",
    "for name, component in nlp.components:\n",
    "    # Have a look at https://spacy.io/usage/spacy-101#pipelines\n",
    "    # The models components are separate pipelines and can be accessed via nlp.get_pipe(<name>).\n",
    "    print(name, component)\n",
    "\n",
    "# Print the last layer of the ner pipeline used for classification.\n",
    "ner = nlp.get_pipe('ner')\n",
    "ner_last_layer = ner.model.layers[-1]\n",
    "print(\"\\tLast layer in ner pipeline:\", ner_last_layer.name)\n",
    "\n",
    "# Test the custom tokenizer on a sample text\n",
    "print(\"\\nNamed entities in example sentence:\")\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extracting the named entities\n",
    "for ent in nlp(sentence).ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model performance on their standard data.\n",
    "for model_name in ['en_core_web_sm', 'en_core_web_md']:\n",
    "    pre_trained_model = spacy.load(model_name)\n",
    "    # get the baseline values for all standard models.\n",
    "    performance = pre_trained_model.meta.get('performance')\n",
    "    pre = performance.get('ents_p')\n",
    "    rec = performance.get('ents_r')\n",
    "    f1 = performance.get('ents_f')\n",
    "    \n",
    "    print(f\"Name:{model_name}:\")\n",
    "    for source in pre_trained_model.meta.get('sources'):\n",
    "        print(f\"Source: {source.get('name')}\")\n",
    "    print(\"NER Performance: \")\n",
    "    print(f\"\\tF1 = {f1:.3f}, Prec. = {pre:.3f}, Rec. {rec:.3f}:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "text = \"What video sharing service did Steve Chen, Chad Hurley, and Jawed Karim create in 2005 and published in Dublin?\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    " \n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">What video sharing service did \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steve Chen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Chad Hurley\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jawed Karim\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " create in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2005\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " and published in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Dublin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "?</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcfeger/Library/Caches/pypoetry/virtualenvs/cir-xscWUHpY-py3.10/lib/python3.10/site-packages/spacy/displacy/__init__.py:211: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Antiretroviral therapy ( ART ) is recommended for all HIV-infected individuals</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "string = \"Antiretroviral therapy ( ART ) is recommended for all HIV-infected individuals\"\n",
    "doc = nlp(string)\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"text\": \"While bismuth compounds (Pepto-Bismol) decreased the number of bowel movements in those with travelers' diarrhea, they do not decrease the length of illness.[91] Anti-motility agents like loperamide are also effective at reducing the number of stools but not the duration of disease.[8] These agents should be used only if bloody diarrhea is not present.[92]\\n\\nDiosmectite, a natural aluminomagnesium silicate clay, is effective in alleviating symptoms of acute diarrhea in children,[93] and also has some effects in chronic functional diarrhea, radiation-induced diarrhea, and chemotherapy-induced diarrhea.[45] Another absorbent agent used for the treatment of mild diarrhea is kaopectate.\\n\\nRacecadotril an antisecretory medication may be used to treat diarrhea in children and adults.[86] It has better tolerability than loperamide, as it causes less constipation and flatulence.[94]\",\n",
      "    \"entities\": [\n",
      "        [\n",
      "            360,\n",
      "            371,\n",
      "            \"MEDICINE\"\n",
      "        ],\n",
      "        [\n",
      "            383,\n",
      "            408,\n",
      "            \"MEDICINE\"\n",
      "        ],\n",
      "        [\n",
      "            104,\n",
      "            112,\n",
      "            \"MEDICALCONDITION\"\n",
      "        ],\n",
      "        [\n",
      "            679,\n",
      "            689,\n",
      "            \"MEDICINE\"\n",
      "        ],\n",
      "        [\n",
      "            6,\n",
      "            23,\n",
      "            \"MEDICINE\"\n",
      "        ],\n",
      "        [\n",
      "            25,\n",
      "            37,\n",
      "            \"MEDICINE\"\n",
      "        ],\n",
      "        [\n",
      "            461,\n",
      "            470,\n",
      "            \"MEDICALCONDITION\"\n",
      "        ],\n",
      "        [\n",
      "            577,\n",
      "            589,\n",
      "            \"MEDICINE\"\n",
      "        ],\n",
      "        [\n",
      "            853,\n",
      "            865,\n",
      "            \"MEDICALCONDITION\"\n",
      "        ],\n",
      "        [\n",
      "            188,\n",
      "            198,\n",
      "            \"MEDICINE\"\n",
      "        ],\n",
      "        [\n",
      "            754,\n",
      "            762,\n",
      "            \"MEDICALCONDITION\"\n",
      "        ],\n",
      "        [\n",
      "            870,\n",
      "            880,\n",
      "            \"MEDICALCONDITION\"\n",
      "        ],\n",
      "        [\n",
      "            823,\n",
      "            833,\n",
      "            \"MEDICINE\"\n",
      "        ],\n",
      "        [\n",
      "            852,\n",
      "            853,\n",
      "            \"MEDICALCONDITION\"\n",
      "        ],\n",
      "        [\n",
      "            461,\n",
      "            469,\n",
      "            \"MEDICALCONDITION\"\n",
      "        ],\n",
      "        [\n",
      "            535,\n",
      "            543,\n",
      "            \"MEDICALCONDITION\"\n",
      "        ],\n",
      "        [\n",
      "            692,\n",
      "            704,\n",
      "            \"MEDICINE\"\n",
      "        ],\n",
      "        [\n",
      "            563,\n",
      "            571,\n",
      "            \"MEDICALCONDITION\"\n",
      "        ]\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "with open('../data/Corona2.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    \n",
    "training_data = {'classes' : ['MEDICINE', \"MEDICALCONDITION\", \"PATHOGEN\"], 'annotations' : []}\n",
    "for example in data['examples']:\n",
    "    temp_dict = {}\n",
    "    temp_dict['text'] = example['content']\n",
    "    temp_dict['entities'] = []\n",
    "    for annotation in example['annotations']:\n",
    "        start = annotation['start']\n",
    "        end = annotation['end']\n",
    "        label = annotation['tag_name'].upper()\n",
    "        temp_dict['entities'].append((start, end, label))\n",
    "        training_data['annotations'].append(temp_dict)\n",
    "    \n",
    "    \n",
    "example_annotations = training_data['annotations'][0]\n",
    "\n",
    "print(json.dumps(example_annotations, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 295/295 [00:00<00:00, 1117.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "doc_bin = DocBin() # create a DocBin object\n",
    "\n",
    "\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "for training_example  in tqdm(training_data['annotations']): \n",
    "    text = training_example['text']\n",
    "    labels = training_example['entities']\n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents \n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"../data/training_data.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-03 09:41:34.038811: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001B[38;5;2m✔ Auto-filled config with all values\u001B[0m\n",
      "\u001B[38;5;2m✔ Saved config\u001B[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-03 09:41:57.041454: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001B[38;5;4mℹ Saving to output directory: .\u001B[0m\n",
      "\u001B[38;5;4mℹ Using GPU: 0\u001B[0m\n",
      "\u001B[1m\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\n",
      "[2023-03-03 09:42:01,293] [INFO] Set up nlp object from config\n",
      "[2023-03-03 09:42:01,304] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2023-03-03 09:42:01,307] [INFO] Created vocabulary\n",
      "[2023-03-03 09:42:01,308] [INFO] Finished initializing nlp object\n",
      "[2023-03-03 09:42:01,982] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001B[38;5;2m✔ Initialized pipeline\u001B[0m\n",
      "\u001B[1m\n",
      "============================= Training pipeline =============================\u001B[0m\n",
      "\u001B[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001B[0m\n",
      "\u001B[38;5;4mℹ Initial learn rate: 0.001\u001B[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     80.71    2.18    1.23    9.22    0.02\n",
      "  0     200        466.90   4901.22   91.03   92.55   89.56    0.91\n",
      "  1     400        172.72    511.18   97.07   97.24   96.91    0.97\n",
      "  2     600       1792.31    291.57   97.97   98.45   97.49    0.98\n",
      "  2     800        296.59    167.81   98.42   98.37   98.47    0.98\n",
      "  3    1000        152.71    190.87   98.47   98.47   98.47    0.98\n",
      "  4    1200       1574.19    222.69   98.21   98.09   98.32    0.98\n",
      "  5    1400        158.20    170.98   98.47   98.47   98.47    0.98\n",
      "  6    1600        169.90    219.60   98.47   98.10   98.85    0.98\n",
      "  7    1800         90.20    125.70   98.44   98.42   98.47    0.98\n",
      "  8    2000        183.43    174.33   98.35   98.35   98.35    0.98\n",
      "  9    2200        216.24    275.56   98.66   98.85   98.47    0.99\n",
      " 11    2400        205.70    237.78   98.66   98.85   98.47    0.99\n",
      " 14    2600        214.22    316.54   98.66   98.85   98.47    0.99\n",
      " 17    2800        192.66    327.79   98.85   98.85   98.85    0.99\n",
      " 20    3000        224.27    354.07   98.85   99.23   98.47    0.99\n",
      " 24    3200        103.33    310.52   98.85   98.85   98.85    0.99\n",
      " 27    3400        152.80    305.82   98.85   98.85   98.85    0.99\n",
      " 30    3600        314.36    329.05   98.85   99.23   98.47    0.99\n",
      " 34    3800        134.10    337.55   98.85   98.85   98.85    0.99\n",
      " 37    4000         83.57    295.62   98.85   98.48   99.23    0.99\n",
      " 41    4200        367.21    340.15   98.85   98.85   98.85    0.99\n",
      " 44    4400        255.50    303.44   98.85   98.85   98.85    0.99\n",
      " 48    4600        139.79    307.01   98.85   99.23   98.47    0.99\n",
      " 51    4800        140.96    296.72   98.85   99.23   98.47    0.99\n",
      " 55    5000         96.04    285.73   98.85   98.85   98.85    0.99\n",
      " 58    5200         50.15    275.58   98.85   98.48   99.23    0.99\n",
      " 62    5400         95.14    272.19   98.85   98.85   98.85    0.99\n",
      " 65    5600        106.72    290.85   98.85   98.85   98.85    0.99\n",
      "\u001B[38;5;2m✔ Saved pipeline to output directory\u001B[0m\n",
      "model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./ --paths.train ./training_data.spacy --paths.dev ./training_data.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7DF6D9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Antiretroviral therapy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICINE</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #7DF6D9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ART\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICINE</span>\n",
       "</mark>\n",
       ") is recommended for all \n",
       "<mark class=\"entity\" style=\"background: #F67DE3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIV\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PATHOGEN</span>\n",
       "</mark>\n",
       "-infectedindividuals to reduce the risk of disease progression.</br>\n",
       "<mark class=\"entity\" style=\"background: #7DF6D9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ART\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICINE</span>\n",
       "</mark>\n",
       " also is recommended for \n",
       "<mark class=\"entity\" style=\"background: #F67DE3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIV\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PATHOGEN</span>\n",
       "</mark>\n",
       "-infected individuals for the prevention of transmission of \n",
       "<mark class=\"entity\" style=\"background: #F67DE3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIV\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PATHOGEN</span>\n",
       "</mark>\n",
       ".</br>Patients starting \n",
       "<mark class=\"entity\" style=\"background: #7DF6D9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ART\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICINE</span>\n",
       "</mark>\n",
       " should be willing and able to commit to treatment and understand thebenefits and risks of therapy and the importance of adherence. Patients may chooseto postpone therapy, and providers, on a case-by-case basis, may elect to defertherapy on the basis of clinical and/or psychosocial factors.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp_ner = spacy.load(\"model-best\")\n",
    "\n",
    "doc = nlp_ner(\"Antiretroviral therapy (ART) is recommended for all HIV-infected\\\n",
    "individuals to reduce the risk of disease progression.\\nART also is recommended \\\n",
    "for HIV-infected individuals for the prevention of transmission of HIV.\\nPatients \\\n",
    "starting ART should be willing and able to commit to treatment and understand the\\\n",
    "benefits and risks of therapy and the importance of adherence. Patients may choose\\\n",
    "to postpone therapy, and providers, on a case-by-case basis, may elect to defer\\\n",
    "therapy on the basis of clinical and/or psychosocial factors.\")\n",
    "\n",
    "colors = {\"PATHOGEN\": \"#F67DE3\", \"MEDICINE\": \"#7DF6D9\", \"MEDICALCONDITION\":\"#FFFFFF\"}\n",
    "options = {\"colors\": colors} \n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", options= options, jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}