Add a description for running the training inside Googel-Colab.

This includes a config file for the transformer.
This also includes a separate command for starting the training with the transformer.

Before the next step, how can the output models be renamed?
Further, use two separate workflows, one for the model with static vectors and one for the foundation model.

Compare the results against each other and explain the advantages of an WordPiece/SentencePiece vocabulary over static vectors in this context.

Also explain how the workflows can be started and how the assets can be loaded.

Add one (or two separate) notebooks using the respective workflows.
Make them ready for Colab and try to show the configurations for each (CPU, GPU, name and configuration).

Remind, for NER the recall seems to be of special interest. On the one-hand-side the recall can be raised artificially by using an external knowledge-base.
On the other side this knowledge base has to be either created, found, researched and has to be put into rules like regex-matching.


ℹ Running workflow 'preparation'

==================================== info ====================================
ℹ Skipping 'info': nothing changed

======================= brace-yourself-data-is-coming =======================
Running command: mkdir -p corpus/CyNER/
Running command: spacy convert assets/CyNER/train.txt ./corpus/CyNER/ --file-type spacy --converter conll
ℹ Grouping every 1 sentences into a document.
⚠ To generate better training data, you may want to group sentences
into documents with `-n 10`.
✔ Generated output file (2811 documents): corpus/CyNER/train.spacy
Running command: spacy convert assets/CyNER/valid.txt ./corpus/CyNER/ --file-type spacy --converter conll
ℹ Grouping every 1 sentences into a document.
⚠ To generate better training data, you may want to group sentences
into documents with `-n 10`.
✔ Generated output file (813 documents): corpus/CyNER/valid.spacy
Running command: spacy convert assets/CyNER/test.txt ./corpus/CyNER/ --file-type spacy --converter conll
ℹ Grouping every 1 sentences into a document.
⚠ To generate better training data, you may want to group sentences
into documents with `-n 10`.
✔ Generated output file (748 documents): corpus/CyNER/test.spacy
Running command: mv corpus/CyNER/valid.spacy corpus/CyNER/dev.spacy

========================= initialize-configurations =========================
ℹ Skipping 'initialize-configurations': nothing changed

################################################################################################################################################################

ℹ Running workflow 'classic-model'

========================== debug-data-classic-model ==========================
Running command: spacy debug data ./configs/training_classic_model_config.cfg --verbose --paths.train ./corpus/CyNER/train.spacy --paths.dev ./corpus/CyNER/dev.spacy

============================ Data file validation ============================
✔ Pipeline can be initialized with data
✔ Corpus is loadable

=============================== Training stats ===============================
Language: en
Training pipeline: tok2vec, ner
2811 training docs
813 evaluation docs
⚠ 12 training examples also in evaluation data

============================== Vocab & Vectors ==============================
ℹ 68191 total word(s) in the data (7954 unique)
10 most common words: 'the' (3674), '.' (2664), ',' (2597), 'to' (1648), 'of'
(1410), 'and' (1256), 'a' (1105), 'is' (787), 'in' (784), 'The' (568)
ℹ 514157 vectors (514157 unique keys, 300 dimensions)
⚠ 2944 words in training data without vectors (4%)
10 most common words without vectors: 'hxxp' (50), 'XLoader' (41), 'FinFisher'
(41), 'RuMMS' (37), 'Asacub' (29), 'Ginp' (21), 'Gooligan' (19), 'GolfSpy' (19),
'HummingBad' (18), 'FrozenCell' (15)

========================== Named Entity Recognition ==========================
ℹ 5 label(s)
0 missing value(s) (tokens with '-' label)
Labels in train data: 'Indicator' (1252), 'System' (838), 'Malware' (705),
'Organization' (288), 'Vulnerability' (48)
⚠ Low number of examples for label 'Vulnerability' (48)
✔ Examples without occurrences available for all labels
✔ No entities consisting of or starting/ending with whitespace
✔ No entities crossing sentence boundaries
To train a new entity type, your data should include at least 50 instances of
the new label

================================== Summary ==================================
✔ 5 checks passed
⚠ 3 warnings

============================ train-classic-model ============================
Running command: spacy train ./configs/training_classic_model_config.cfg --gpu-id 0 --verbose --output ./models/classic --paths.train ./corpus/CyNER/train.spacy --paths.dev ./corpus/CyNER/dev.spacy
[2023-03-06 20:24:18,620] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']
✔ Created output directory: models/classic
ℹ Saving to output directory: models/classic
ℹ Using GPU: 0

=========================== Initializing pipeline ===========================
[2023-03-06 20:24:23,663] [INFO] Set up nlp object from config
[2023-03-06 20:24:23,673] [DEBUG] Loading corpus from path: corpus/CyNER/dev.spacy
[2023-03-06 20:24:23,674] [DEBUG] Loading corpus from path: corpus/CyNER/train.spacy
[2023-03-06 20:24:23,674] [INFO] Pipeline: ['tok2vec', 'ner']
[2023-03-06 20:24:23,677] [INFO] Created vocabulary
[2023-03-06 20:24:26,484] [INFO] Added vectors: en_core_web_lg
[2023-03-06 20:24:28,952] [INFO] Finished initializing nlp object
[2023-03-06 20:24:40,741] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, id, lb, mk, pt, ru, sr, ta, th

Load the table in your config with:

[initialize.lookups]
@misc = "spacy.LookupsDataLoader.v1"
lang = ${nlp.lang}
tables = ["lexeme_norm"]

[2023-03-06 20:24:44,667] [INFO] Initialized pipeline components: ['tok2vec', 'ner']
✔ Initialized pipeline

============================= Training pipeline =============================
[2023-03-06 20:24:44,677] [DEBUG] Loading corpus from path: corpus/CyNER/dev.spacy
[2023-03-06 20:24:44,678] [DEBUG] Loading corpus from path: corpus/CyNER/train.spacy
ℹ Pipeline: ['tok2vec', 'ner']
ℹ Initial learn rate: 0.001
E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE
---  ------  ------------  --------  ------  ------  ------  ------
  0       0          0.00     50.55    0.00    0.00    0.00    0.00
  0     200         40.49   1622.70   35.01   45.51   28.44    0.35
  0     400         48.77    977.55   52.47   66.41   43.37    0.52
  1     600        128.34    993.97   50.08   68.67   39.41    0.50
  1     800         89.92    803.48   56.67   74.33   45.79    0.57
  2    1000         68.73    896.76   53.04   76.63   40.56    0.53
  3    1200        269.71   1015.75   53.91   73.41   42.60    0.54
  4    1400         94.75    833.22   58.08   72.74   48.34    0.58
  5    1600        123.97    827.46   54.90   69.40   45.41    0.55
  6    1800        132.99    802.54   55.54   73.12   44.77    0.56
  8    2000        200.65    825.76   54.77   70.85   44.64    0.55
 11    2200        172.93    706.84   51.92   69.83   41.33    0.52
 13    2400        247.53    694.28   53.85   69.06   44.13    0.54
 16    2600        186.50    504.66   58.44   73.88   48.34    0.58
 19    2800        231.10    385.22   53.83   70.60   43.49    0.54
 22    3000        276.03    370.87   55.58   70.10   46.05    0.56
 25    3200        305.49    289.98   51.91   62.75   44.26    0.52
 28    3400        358.81    357.17   55.18   64.51   48.21    0.55
 31    3600        290.54    258.08   53.12   63.75   45.54    0.53
 33    3800        284.61    261.71   55.05   69.59   45.54    0.55
 36    4000        345.57    216.12   55.12   70.44   45.28    0.55
 39    4200        344.40    217.18   55.79   63.92   49.49    0.56
✔ Saved pipeline to output directory
models/classic/model-last

=========================== evaluate-classic-model ===========================
Running command: spacy evaluate ./models/classic/model-best ./corpus/CyNER/test.spacy --gpu-id 0
ℹ Using GPU: 0

================================== Results ==================================

TOK     -
NER P   70.34
NER R   50.43
NER F   58.74
SPEED   13330


=============================== NER (per type) ===============================

                    P       R       F
System          68.32   44.18   53.66
Organization    60.94   29.10   39.39
Vulnerability   80.00   40.00   53.33
Malware         78.57   45.45   57.59
Indicator       69.44   69.44   69.44

################################################################################################################################################################

ℹ Running workflow 'foundation-model'

======================== debug-data-foundation-model ========================
Running command: spacy debug data ./configs/training_foundation_model_config.cfg --verbose --paths.train ./corpus/CyNER/train.spacy --paths.dev ./corpus/CyNER/dev.spacy

============================ Data file validation ============================
Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 78.4kB/s]
Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:01<00:00, 791kB/s]
Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 509kB/s]
Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.19MB/s]
Downloading (…)"pytorch_model.bin";: 100% 501M/501M [00:01<00:00, 320MB/s]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
✔ Pipeline can be initialized with data
✔ Corpus is loadable

=============================== Training stats ===============================
Language: en
Training pipeline: transformer, ner
2811 training docs
813 evaluation docs
⚠ 12 training examples also in evaluation data

============================== Vocab & Vectors ==============================
ℹ 68191 total word(s) in the data (7954 unique)
10 most common words: 'the' (3674), '.' (2664), ',' (2597), 'to' (1648), 'of'
(1410), 'and' (1256), 'a' (1105), 'is' (787), 'in' (784), 'The' (568)
ℹ No word vectors present in the package

========================== Named Entity Recognition ==========================
ℹ 5 label(s)
0 missing value(s) (tokens with '-' label)
Labels in train data: 'Indicator' (1252), 'System' (838), 'Malware' (705),
'Organization' (288), 'Vulnerability' (48)
⚠ Low number of examples for label 'Vulnerability' (48)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
✔ Examples without occurrences available for all labels
✔ No entities consisting of or starting/ending with whitespace
✔ No entities crossing sentence boundaries
To train a new entity type, your data should include at least 50 instances of
the new label

================================== Summary ==================================
✔ 5 checks passed
⚠ 2 warnings

=========================== train-foundation-model ===========================
Running command: spacy train ./configs/training_foundation_model_config.cfg --gpu-id 0 --verbose --output ./models/foundation --paths.train ./corpus/CyNER/train.spacy --paths.dev ./corpus/CyNER/dev.spacy
[2023-03-06 20:42:25,650] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']
✔ Created output directory: models/foundation
ℹ Saving to output directory: models/foundation
ℹ Using GPU: 0

=========================== Initializing pipeline ===========================
[2023-03-06 20:42:27,860] [INFO] Set up nlp object from config
[2023-03-06 20:42:27,874] [DEBUG] Loading corpus from path: corpus/CyNER/dev.spacy
[2023-03-06 20:42:27,876] [DEBUG] Loading corpus from path: corpus/CyNER/train.spacy
[2023-03-06 20:42:27,876] [INFO] Pipeline: ['transformer', 'ner']
[2023-03-06 20:42:27,882] [INFO] Created vocabulary
[2023-03-06 20:42:27,883] [INFO] Finished initializing nlp object
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[2023-03-06 20:42:35,530] [DEBUG] [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed and load the table in your config. The languages with lexeme normalization tables are currently: cs, da, de, el, en, id, lb, mk, pt, ru, sr, ta, th

Load the table in your config with:

[initialize.lookups]
@misc = "spacy.LookupsDataLoader.v1"
lang = ${nlp.lang}
tables = ["lexeme_norm"]

[2023-03-06 20:42:49,903] [INFO] Initialized pipeline components: ['transformer', 'ner']
✔ Initialized pipeline

============================= Training pipeline =============================
[2023-03-06 20:42:49,914] [DEBUG] Loading corpus from path: corpus/CyNER/dev.spacy
[2023-03-06 20:42:49,915] [DEBUG] Loading corpus from path: corpus/CyNER/train.spacy
ℹ Pipeline: ['transformer', 'ner']
ℹ Initial learn rate: 0.0
E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE
---  ------  -------------  --------  ------  ------  ------  ------
  0       0          99.65     73.35    0.49    0.27    3.06    0.00
  3     200       48342.74  43532.91   69.72   70.59   68.88    0.70
  7     400        3346.25   3369.57   77.96   78.10   77.81    0.78
 11     600        2114.85   1559.36   75.99   76.49   75.51    0.76
 15     800         914.59    830.84   77.99   76.70   79.34    0.78
 18    1000         457.99    486.05   74.56   79.39   70.28    0.75
 22    1200         295.97    319.73   76.81   75.71   77.93    0.77
 26    1400         227.98    288.84   75.95   81.25   71.30    0.76
 30    1600         177.33    212.14   77.20   77.75   76.66    0.77
 34    1800         144.28    166.14   76.35   79.18   73.72    0.76
 37    2000         161.82    170.65   76.51   81.53   72.07    0.77
 41    2200         165.31    141.49   78.50   79.48   77.55    0.79
 45    2400         159.92    130.05   79.23   82.87   75.89    0.79
 49    2600         136.20     99.16   76.43   76.97   75.89    0.76
 52    2800          79.86     74.80   76.48   78.86   74.23    0.76
 56    3000          80.98     77.50   75.99   77.56   74.49    0.76
 60    3200          63.43     71.89   78.66   79.53   77.81    0.79
 64    3400          99.32    104.08   77.99   78.70   77.30    0.78
 67    3600          78.10     77.82   75.90   76.29   75.51    0.76
 71    3800          74.51     69.43   76.26   82.81   70.66    0.76
 75    4000          67.26     69.58   74.77   75.45   74.11    0.75
✔ Saved pipeline to output directory
models/foundation/model-last

========================= evaluate-foundation-model =========================
Running command: spacy evaluate ./models/foundation/model-best ./corpus/CyNER/test.spacy --gpu-id 0
ℹ Using GPU: 0

================================== Results ==================================

TOK     -
NER P   77.14
NER R   71.37
NER F   74.14
SPEED   3600


=============================== NER (per type) ===============================

                      R       P      F
System            63.86   70.04  66.81
Organization      44.03   71.95  54.63
Malware           80.58   79.27  79.92
Vulnerability     70.00  100.00  82.35
Indicator         82.39   81.58  81.98
